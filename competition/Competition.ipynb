{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: distance in c:\\users\\b_tib\\anaconda3\\lib\\site-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install distance"
   ]
  },
  {
   "source": [
    "## Importing libraries..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NzTRBPwN8KPX"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "import collections \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# models\n",
    "from sklearn import  svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Feature selection\n",
    "from sklearn import feature_selection\n",
    "from sklearn import preprocessing,  svm\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# validation\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Some function to help on data cleaning/processing..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUXILIARY FUNCTIONS\n",
    "\n",
    "\n",
    "## FOR CATEGORIES:\n",
    "def alpha_views(dataframe, views_ref=None):\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    cols = df.columns\n",
    "    column_values = df.values.ravel()\n",
    "    unique_values =  pd.unique(column_values)\n",
    "    \n",
    "    if not views_ref:\n",
    "        Training = True\n",
    "        alpha = {}\n",
    "        for value, label in zip(unique_values, product_gen(string.ascii_uppercase)):\n",
    "            alpha[value] = label    \n",
    "\n",
    "        for col in cols:\n",
    "            df.replace({col: alpha}, inplace = True)\n",
    "\n",
    "        return df, alpha\n",
    "    \n",
    "    # Rename test views according to views_ref\n",
    "    diff_views = [i for i in list(views_ref) + unique_values.tolist() if i not in views_ref.keys()]\n",
    "    upd_views = list(views_ref) + diff_views\n",
    "    alpha = {}\n",
    "    for value, label in zip(upd_views, product_gen(string.ascii_uppercase)):\n",
    "            alpha[value] = label    \n",
    "\n",
    "    for col in cols:\n",
    "        df.replace({col: alpha}, inplace = True)\n",
    "    return df\n",
    "\n",
    "def product_gen(n):\n",
    "    '''\n",
    "    Code reference:\n",
    "    https://stackoverflow.com/questions/6412473/python-assign-letter-of-the-alphabet-to-each-value-in-a-list'''\n",
    "    \n",
    "    for r in itertools.count(1):\n",
    "        for i in itertools.product(n, repeat=r):\n",
    "            yield \"\".join(i)\n",
    "\n",
    "\n",
    "def dict_categ_sequences(sequence, cluster_ref=None, damping=None):\n",
    "    ''' Return a dictionaire clustering strings listed by levenshtein-distancing. \n",
    "    Code reference:\n",
    "    https://stats.stackexchange.com/questions/123060/clustering-a-long-list-of-strings-words-into-similarity-groups '''\n",
    "    \n",
    "    if not cluster_ref:\n",
    "        Training = True\n",
    "         ## Affinity clustering by string distance  \n",
    "        sequence = np.asarray(sequence)\n",
    "        lev_similarity = -1*np.array([[distance.levenshtein(w1, w2) for w1 in sequence] for w2 in sequence])\n",
    "        affprop = AffinityPropagation(affinity=\"precomputed\", damping=damping) ## To explore\n",
    "        affprop.fit(lev_similarity)\n",
    "       \n",
    "        ## Run dictionaire d        \n",
    "        d = {}  \n",
    "        i = 0\n",
    "        exemplars = []\n",
    "        for cluster_id in np.unique(affprop.labels_):\n",
    "            exemplar = sequence[affprop.cluster_centers_indices_[cluster_id]]\n",
    "            cluster = np.unique(sequence[np.nonzero(affprop.labels_==cluster_id)])\n",
    "            exemplars.append(exemplar)        \n",
    "            key = 'seq_' + str(i)\n",
    "            d[key] = cluster\n",
    "            i += 1\n",
    "                \n",
    "        ## Assembling output from d\n",
    "        newdict = {}\n",
    "        newdict = {i: k for k, v in d.items() for i in v}   \n",
    "        \n",
    "        return newdict, exemplars\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        # for each sequence, calculate the closest reference\n",
    "        reference_list = [ cluster_ref[np.argmin([distance.levenshtein(w1, w2) for w1 in cluster_ref])] for w2 in sequence]\n",
    "        \n",
    "        # create a dictionaire with reference \"keys\" and sequence \"values\"\n",
    "        dct ={k:[x for id,x in enumerate(sequence) if k == reference_list[id]] for k in reference_list}\n",
    "        \n",
    "        # Add missing references to dct and rename keys to seq_():\n",
    "        new_dct ={}\n",
    "        for i, cluster in enumerate(cluster_ref):\n",
    "            if cluster in dct.keys():\n",
    "                new_dct['seq_'+str(i)] = dct[cluster]\n",
    "            else:\n",
    "                new_dct['seq_'+str(i)] = []\n",
    "                \n",
    "        # assembling output form new_dct\n",
    "        newdict = {} \n",
    "        newdict = {i: k for k, v in new_dct.items() for i in v}  \n",
    "        \n",
    "        return newdict\n",
    "\n",
    "\n",
    "def new_categ_features(df, cluster_ref=None, Training=True, damping=0.98):       \n",
    "    '''Generates new features based on the task interpretation'''       \n",
    "    \n",
    "    dataframe = df.copy()\n",
    "    \n",
    "    if cluster_ref:\n",
    "        Training = False\n",
    "    ## Redudancy\n",
    "    dataframe['not_na'] = dataframe.apply(lambda x: x.count()/6, axis=1)    \n",
    "    \n",
    "    ## Diversity\n",
    "    dataframe['diversity'] = [len(set(v[pd.notna(v)]))/6 for v in dataframe.iloc[:,:-1].values]   \n",
    "            \n",
    "    if Training:\n",
    "        ## Sequence\n",
    "        dataframe['sequence'] = dataframe[dataframe.columns[:6]].apply(lambda x: ''.join(x.dropna().astype(str)), axis=1)\n",
    "        sequences = dataframe['sequence'].values\n",
    "        newdict, keys = dict_categ_sequences(sequences, damping=damping)\n",
    "        dataframe['seq_categ'] = dataframe.sequence.str.findall('|'.join(newdict.keys())).str[0].map(newdict)        \n",
    "        \n",
    "        ## Diversity factor\n",
    "        dataframe['divesity_factor'] = round(dataframe.diversity * dataframe.not_na,2)        \n",
    "        ## Drop auxiliars columns\n",
    "        dataframe = dataframe.drop(['diversity', 'not_na', 'sequence'], axis=1)\n",
    "                \n",
    "        return dataframe, keys\n",
    "        \n",
    "    else:\n",
    "        ## Sequence\n",
    "        dataframe['sequence'] = dataframe[dataframe.columns[:6]].apply(lambda x: ''.join(x.dropna().astype(str)), axis=1)\n",
    "        sequences = set(dataframe['sequence'].values)\n",
    "        newdict = dict_categ_sequences(sequences, cluster_ref)\n",
    "        dataframe['seq_categ'] = dataframe.sequence.str.findall('|'.join(newdict.keys())).str[0].map(newdict)   \n",
    "        \n",
    "        ## Diversity factor\n",
    "        dataframe['divesity_factor'] = round(dataframe.diversity * dataframe.not_na,2)\n",
    "        ## Drop auxiliars columns\n",
    "        dataframe = dataframe.drop(['diversity', 'not_na', 'sequence'], axis=1)        \n",
    "    \n",
    "        return dataframe\n",
    "\n",
    "def dummy_new_features(dataframe, valid_cols=None, Training = True):\n",
    "    '''Convert new created categories into dummy variables'''\n",
    "    dm_dataframe = dataframe.copy()\n",
    "    \n",
    "    if valid_cols is not None:\n",
    "        Training = False\n",
    "    \n",
    "    if Training:        \n",
    "        dm_dataframe = pd.get_dummies(dataframe.iloc[:,:-1].astype(str), prefix='', prefix_sep='').max(level=0, axis=1)\n",
    "        dm_dataframe = dm_dataframe.reindex(sorted(dm_dataframe.columns), axis=1)\n",
    "        dm_dataframe['divesity_factor'] = dataframe['divesity_factor']\n",
    "        dm_dataframe = dm_dataframe.iloc[:,:-1].multiply(dm_dataframe['divesity_factor'], axis=\"index\")\n",
    "        dm_dataframe['divesity_factor'] = dataframe.divesity_factor*6\n",
    "        \n",
    "        return dm_dataframe, dm_dataframe.iloc[:,:-1].columns\n",
    "        \n",
    "    else:\n",
    "        dm_dataframe = pd.get_dummies(dataframe.iloc[:,:-1].astype(str), prefix='', prefix_sep='').max(level=0, axis=1)\n",
    "        dm_dataframe = dm_dataframe.reindex(sorted(dm_dataframe.columns), axis=1)        \n",
    "        test_cols = dm_dataframe.columns        \n",
    "        # list columns to remove (which are in  test_cols no in training_views values):\n",
    "        to_drop = []\n",
    "        for col in test_cols:\n",
    "            if 'seq_' not in col:\n",
    "                if col not in valid_cols:\n",
    "                    to_drop.append(col)\n",
    "        dm_dataframe = dm_dataframe.drop(to_drop, axis=1)\n",
    "        # list columns to add (which are not in test_cols but in training_views values):\n",
    "        to_add = []\n",
    "        for col in valid_cols:\n",
    "            if col not in test_cols:\n",
    "                dm_dataframe[col] = 0\n",
    "                \n",
    "        dm_dataframe = dm_dataframe.reindex(sorted(dm_dataframe.columns), axis=1)\n",
    "        dm_dataframe['divesity_factor'] = dataframe['divesity_factor']\n",
    "        dm_dataframe = dm_dataframe.iloc[:,:-1].multiply(dm_dataframe['divesity_factor'], axis=\"index\")\n",
    "        dm_dataframe['divesity_factor'] = dataframe.divesity_factor*6\n",
    "        \n",
    "    return dm_dataframe\n",
    "\n",
    "## FOR SCALARS:\n",
    "def list_constant_columns(dataframe): \n",
    "    '''List columns with no variance'''\n",
    "    to_drop = []\n",
    "    result = dataframe.copy()\n",
    "    for column in dataframe.columns:\n",
    "        \n",
    "        if dataframe[column].std() == 0:\n",
    "            to_drop.append(column)\n",
    "    return to_drop\n",
    "\n",
    "def list_lcorr_columns(dataframe, threshold=0.1):\n",
    "    '''List columns with 'Label-Correlation' lower than threshold'''\n",
    "    to_drop = []\n",
    "    for col in dataframe.columns:\n",
    "        corr = dataframe[col].corr(dataframe['labels'])\n",
    "        if abs(corr) < threshold:\n",
    "            to_drop.append(col)\n",
    "    return to_drop\n",
    "\n",
    "\n",
    "def replace_nan_norm(dataframe, norm_list=None, Training=True):\n",
    "    '''Replace Nan values and normalize according to the training set'''\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    if norm_list:\n",
    "        Training = False\n",
    "        \n",
    "    if Training:\n",
    "        norm_list = []\n",
    "        for col in df.iloc[:,:-1].columns:\n",
    "            mean = np.nanmean(df[col]) \n",
    "            std = np.std(df[col])\n",
    "            df[col] = df[col].fillna(mean)\n",
    "            df[col] =(df[col]-mean )/std            \n",
    "            norm_list.append([mean, std])\n",
    "            \n",
    "        return df, norm_list\n",
    "        \n",
    "    else:\n",
    "        assert dataframe.shape[1] == len(norm_list), 'Different sizes: df={} meanlist={} size board.'.format(dataframe.shape[1], len(mean_list))\n",
    "        for id, col in enumerate(df.columns):\n",
    "            df[col] = df[col].fillna(norm_list[id][0])\n",
    "            df[col] = (df[col]-norm_list[id][0] )/norm_list[id][1]\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "source": [
    "## Importing the datasets..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "5t8AkmZTS5Ga"
   },
   "outputs": [],
   "source": [
    "test_data = pickle.load(open(\"C:/Users/b_tib/coding/Msc/oLINGI2262/Inginious_task_5/test.pickle\", \"rb\"))\n",
    "(train_data_1, train_labels_1) = pickle.load(open(\"C:/Users/b_tib/coding/Msc/oLINGI2262/Inginious_task_5/train1.pickle\", \"rb\"))\n",
    "(train_data_2, train_labels_2) = pickle.load(open(\"C:/Users/b_tib/coding/Msc/oLINGI2262/Inginious_task_5/train2.pickle\", \"rb\"))"
   ]
  },
  {
   "source": [
    "## Splitting categorical and scalar values ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING 1 & 2:\n",
    "train_data = train_data_1.append(train_data_2)\n",
    "train_labels =np.concatenate((train_labels_1, train_labels_2), axis=None)\n",
    "\n",
    "# SEPARTING SCALAR:\n",
    "train_data_scalar = train_data.iloc[:,:1306624]\n",
    "test_data_scalar = test_data.iloc[:,:1306624] # Test (no labels)\n",
    "#train_data_scalar['labels'] = train_labels\n",
    "\n",
    "## Categories\n",
    "train_data_categ_raw = train_data.iloc[:,1306624:]\n",
    "test_data_categ_raw= test_data.iloc[:,1306624:] # Test"
   ]
  },
  {
   "source": [
    "## Processing\n",
    "### 1) Divide dataframe into subsets by:\n",
    "#### - Damping categorical according to damping coeficients\n",
    "#### - Renmoving columns with low target-correlation according to threshold\n",
    "### 2) Concatenate damping/thresholds subsets into dct_subsets:\n",
    "#### - Store possible pairwises into a dictionaire\n",
    "#### - For each pairwise, run most importante features (run in google colab)\n",
    "### 3) Run the most relevant features for each dct_subsets and stored in dct_features:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# THIS STEP WAS COMPUTED IN GOOGLE COLAB\n",
    "# THE MOST IMPORTA COLUMNS NAMES ARE STORED IN \"dct_features\".HOWEVER, THERE IS MORE...\n",
    "# THE FUNCTION REPLAN NAN AND NORMILIZE EXTEND TRAIN-SET KNOWNLEDGE TO TEST-SET BEFORE PREDICTION \n",
    "\n",
    "# PROCESSING CATEGORIES\n",
    "dampings = [0.9] #  0.95, 0.98\n",
    "dct_categ = {}\n",
    "for damping in dampings:\n",
    "  try:\n",
    "      ## Convert views into alphabethic variable for distancing\n",
    "      train_data_angles, training_views = alpha_views(train_data_angles) # for colab: remove \"-\" \n",
    "      test_data_angles = alpha_views(test_data_angles, training_views) # for colab: remove \"-\" \n",
    "\n",
    "      ## Applying edit-distancing and clustering using AffinityPropagation\n",
    "      nf_train_data_angles, keys = new_categ_features(train_data_angles, damping=damping) \n",
    "      nf_test_data_angles = new_categ_features(test_data_angles, keys)\n",
    "\n",
    "      ## Converting new categories in dummy variables\n",
    "      train_data_categ, valid_columns = dummy_new_features(nf_train_data_angles) \n",
    "      test_data_categ = dummy_new_features(nf_test_data_angles, valid_cols=valid_columns)\n",
    "\n",
    "      dct_categ[damping] = [train_data_categ, test_data_categ]\n",
    "      \n",
    "  except Exception:\n",
    "      pass  # or you could use 'continue'\n",
    "\n",
    "# PROCESSING SCALARS\n",
    "\n",
    "thresholds = [0.1] #0.2, 0.3\n",
    "dct_scalar = {}\n",
    "for threshold in thresholds:\n",
    "  try:\n",
    "      ## listing columns to drop based on low correlation threshold based on train-set:\n",
    "      drop_constants = list_constant_columns(train_data_vector) # for colab: remove \"-\" \n",
    "      drop_low_corr = list_lcorr_columns(train_data_vector, threshold = threshold) # for colab: remove \"-\"  \n",
    "\n",
    "      ## Dropping columns to drop based on low correlation threshold:\n",
    "      dp_train_data_vector = train_data_vector.drop(drop_constants, axis=1) # for colab: remove \"-\" \n",
    "      dp_train_data_vector = dp_train_data_vector.drop(drop_low_corr, axis=1)\n",
    "\n",
    "      dp_test_data_vector = test_data_vector.drop(drop_constants, axis=1) # for colab: remove \"-\" \n",
    "      dp_test_data_vector = dp_test_data_vector.drop(drop_low_corr, axis=1)\n",
    "\n",
    "      ## Replacing Nan values and normalizing columns based on train-set:\n",
    "      train_data_scalar, norm_list = replace_nan_norm(dp_train_data_vector)\n",
    "      test_data_scalar = replace_nan_norm(dp_test_data_vector, norm_list=norm_list)\n",
    "\n",
    "      dct_scalar[threshold] = [train_data_scalar, test_data_scalar]\n",
    "\n",
    "  except Exception:\n",
    "      pass  # or you could use 'continue'\n",
    "\n",
    "\n",
    "# Concatenating train and test SUBSETS\n",
    "dct_subsets = {}\n",
    "for k_categ, v_categ in dct_categ.items():\n",
    "  for k_scalar, v_scalar in dct_scalar.items():\n",
    "    # merger keys\n",
    "    k_db = str(k_categ) + '_' + str(k_scalar)\n",
    "\n",
    "    # merge values\n",
    "    dct_subsets[k_db] = [pd.concat([v_categ[0], v_scalar[0]], axis=1), pd.concat([v_categ[1], v_scalar[1]], axis=1)]\n",
    "\n",
    "\n",
    "# Passing dataframe through Decision tree importance and getting most significant features for each. \n",
    "dct_features = {}\n",
    "\n",
    "for k, v in dct_subsets.items():\n",
    "\n",
    "  train = v[0]\n",
    "\n",
    "  # DECISION TREE Best Features\n",
    "  x_train = train.iloc[:,:-1]\n",
    "  Y_train = train.iloc[:,-1:]\n",
    "  clf = DecisionTreeClassifier(random_state=0)\n",
    "  clf = clf.fit(x_train, Y_train)\n",
    "  dct= dict(zip(x_train.columns, clf.feature_importances_))\n",
    "  dct = {k: v for k, v in sorted(dct.items(), key=lambda item: item[1], reverse=True)}\n",
    "  features = {x:y for x,y in dct.items() if y!=0}\n",
    "  if features not in dct_features.values():\n",
    "    dct_features[k] = features"
   ]
  },
  {
   "source": [
    "## Feature Selection\n",
    "### - Apply damping, threshold and relevant features for a clean experiemntal database.\n",
    "### - Using raw (train/test) data, remember to replace Nan by the mean only (standardization during the cross validations).\n",
    "### - *In the raw test data, the Nan replacements is according to train data mean. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "Rr7v5ihR6ujK"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8 0.1\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.8 0.2\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.8 0.3\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 37)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 37)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.95 0.1\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.95 0.2\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.95 0.3\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 34)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 34)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.98 0.1\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.98 0.2\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 21)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 21)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.98 0.3\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 37)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 37)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.9 0.1\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.9 0.2\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.9 0.3\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 41)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 41)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n"
     ]
    }
   ],
   "source": [
    "dct_databases = {}\n",
    "i=0\n",
    "\n",
    "\n",
    "for k, v in dct_features.items():\n",
    "    \n",
    "    damping = float(k.split('_')[0])\n",
    "    threshold = float(k.split('_')[1])\n",
    "    \n",
    "    # PROCESSING CATEGORIES\n",
    "\n",
    "    ## Convert views into alphabethic variable for distancing\n",
    "    train_data_angles, training_views = alpha_views(train_data_categ_raw) \n",
    "    test_data_angles = alpha_views(test_data_categ_raw, training_views) \n",
    "\n",
    "    ## Applying edit-distancing and clustering using AffinityPropagation\n",
    "    nf_train_data_angles, keys = new_categ_features(train_data_angles, damping=damping) # v.split('_')[0]\n",
    "    nf_test_data_angles = new_categ_features(test_data_angles, keys)\n",
    "\n",
    "    ## Converting new categories in dummy variables\n",
    "    train_data_categ, valid_columns = dummy_new_features(nf_train_data_angles) \n",
    "    test_data_categ = dummy_new_features(nf_test_data_angles, valid_cols=valid_columns)\n",
    "    \n",
    "    # MERGING CATEGORIES & SCALARS    \n",
    "    x_train = pd.concat([train_data_categ, train_data], axis=1) # merges into train data because the scalar will be selected autommaticalli by the features\n",
    "    x_test = pd.concat([test_data_categ, test_data], axis=1) # merges into train data because the scalar will be selected autommaticalli by the features\n",
    "    \n",
    "\n",
    "    # FEATURE SELECTION ON RAW DATA \n",
    "    x_train = x_train[v]\n",
    "    x_test = x_test[v]\n",
    "    \n",
    "\n",
    "    # SELECT FEATURES PROCESSING: Remove Nan & \"\"NOT\"\" Normalize    \n",
    "    x_train, mean_list = replace_nan(x_train)\n",
    "    x_test = replace_nan(x_test, mean_list=mean_list)\n",
    "    Y_train = train_labels \n",
    "\n",
    "    \n",
    "    # check all columns are equal between x_train & x_test\n",
    "    assert x_train.columns.all() == x_test.columns.all(), 'train={} and test={} columsn are different.'.format(x_test.shape[1], x_test.shape[1])\n",
    "    \n",
    "    dct_databases[i] = [damping, threshold, x_train, Y_train, x_test]\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "    print('x_train:',x_train.shape)\n",
    "    print('\\tThe number of missing Values:',x_train.isna().sum().sum())\n",
    "    print('\\tThe average number of missing Values for each attribute:',np.mean(x_train.isnull().sum()))\n",
    "    print('\\tThe standard deviation of the average number of missing Values for each attribute:',np.std(x_train.isnull().sum()))\n",
    "\n",
    "    print('x_test:',x_test.shape)\n",
    "    print('\\tThe number of missing Values:',x_test.isna().sum().sum())\n",
    "    print('\\tThe average number of missing Values for each attribute:',np.mean(x_test.isnull().sum()))\n",
    "    print('\\tThe standard deviation of the average number of missing Values for each attribute:',np.std(x_test.isnull().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_features={'0.8_0.1': {35263: 0.026188480092357164,\n",
    "  45658: 0.02973821520382632,\n",
    "  162020: 0.12808588780188707,\n",
    "  167957: 0.01207678220475255,\n",
    "  188664: 0.01150169733785957,\n",
    "  264555: 0.046907453111410126,\n",
    "  339787: 0.0808855475498934,\n",
    "  362546: 0.0621665799892215,\n",
    "  404039: 0.021469835030671194,\n",
    "  406583: 0.060227069696428284,\n",
    "  501756: 0.04925181091133322,\n",
    "  594089: 0.06937725947411037,\n",
    "  761990: 0.14351154614285724,\n",
    "  846307: 0.012816177033614953,\n",
    "  852873: 0.038585813643910685,\n",
    "  903269: 0.05513940665794529,\n",
    "  1001789: 0.013711827414546322,\n",
    "  1080786: 0.03105458281222084,\n",
    "  1118650: 0.09695250028708043,\n",
    "  1294587: 0.010351527604073612},\n",
    " '0.8_0.2': {4117: 0.04968733249955333,\n",
    "  6179: 0.030611798261165758,\n",
    "  10474: 0.03943439087266137,\n",
    "  16474: 0.017148284437409924,\n",
    "  31899: 0.013479400610422673,\n",
    "  137221: 0.06686668222761828,\n",
    "  256125: 0.10306616214468638,\n",
    "  437665: 0.04622019308329932,\n",
    "  594186: 0.03505279188681011,\n",
    "  633092: 0.07368754094455365,\n",
    "  659771: 0.02628959391510759,\n",
    "  678530: 0.02555392890257758,\n",
    "  679957: 0.011830317261798414,\n",
    "  760039: 0.013693359350270556,\n",
    "  761990: 0.14351154614285724,\n",
    "  803407: 0.03312488833303555,\n",
    "  809822: 0.026151227631343876,\n",
    "  938706: 0.018402715740575306,\n",
    "  1164254: 0.05074450978677787,\n",
    "  1235905: 0.05280518609369591,\n",
    "  1290671: 0.009201357870287653,\n",
    "  1292566: 0.11343679200349172},\n",
    " '0.8_0.3': {1028116: 0.006395981466978193,\n",
    "  1036559: 0.03875671055104585,\n",
    "  1073172: 0.05578637537163887,\n",
    "  1118502: 0.013075613815671938,\n",
    "  1120458: 0.025926432918871443,\n",
    "  120689: 0.023536104868209475,\n",
    "  1209066: 0.024646494295413355,\n",
    "  1210206: 0.026151227631343876,\n",
    "  1212254: 0.013916497942353837,\n",
    "  1292684: 0.025303734143291044,\n",
    "  1294888: 0.006901018402715741,\n",
    "  1294941: 0.053707250405481625,\n",
    "  251297: 0.004495319875560348,\n",
    "  298401: 0.012554679477118974,\n",
    "  339988: 0.02954658249459037,\n",
    "  374177: 0.012735322347293264,\n",
    "  393373: 0.1089777620343429,\n",
    "  397374: 0.0230936040666043,\n",
    "  437665: 0.03342011208408998,\n",
    "  43927: 0.043662839292922306,\n",
    "  464289: 0.015104705974537223,\n",
    "  535969: 0.014492353029623685,\n",
    "  59263: 0.027834107557620155,\n",
    "  639081: 0.01237785300561019,\n",
    "  641101: 0.031027314350447168,\n",
    "  676121: 0.0167789467046422,\n",
    "  721707: 0.006901018402715741,\n",
    "  761990: 0.14954210053237535,\n",
    "  807267: 0.006901018402715741,\n",
    "  855052: 0.009201357870287653,\n",
    "  894893: 0.006591278611008029,\n",
    "  899769: 0.019717195436330687,\n",
    "  899778: 0.013802036805431482,\n",
    "  899956: 0.009201357870287653,\n",
    "  929185: 0.03331456544702834,\n",
    "  984907: 0.03542176864351336,\n",
    "  'D': 0.009201357870287653},\n",
    " '0.95_0.1': {14695: 0.013711827414546318,\n",
    "  45658: 0.029738215203826312,\n",
    "  97348: 0.011501697337859568,\n",
    "  108028: 0.0492518109113332,\n",
    "  162020: 0.12808588780188704,\n",
    "  232378: 0.01035152760407361,\n",
    "  264555: 0.04690745311141012,\n",
    "  315124: 0.012076782204752546,\n",
    "  339787: 0.08088554754989338,\n",
    "  362546: 0.06216657998922149,\n",
    "  406583: 0.06022706969642827,\n",
    "  594089: 0.06937725947411036,\n",
    "  602919: 0.031054582812220833,\n",
    "  620427: 0.01281617703361495,\n",
    "  696765: 0.021469835030671187,\n",
    "  761990: 0.14351154614285722,\n",
    "  846810: 0.02618848009235716,\n",
    "  903269: 0.05513940665794528,\n",
    "  1118650: 0.0969525002870804,\n",
    "  1167222: 0.03858581364391068},\n",
    " '0.95_0.2': {4117: 0.04968733249955332,\n",
    "  6179: 0.030611798261165755,\n",
    "  67058: 0.013479400610422671,\n",
    "  69874: 0.018402715740575306,\n",
    "  137221: 0.06686668222761828,\n",
    "  159934: 0.013693359350270554,\n",
    "  256125: 0.10306616214468636,\n",
    "  291201: 0.009201357870287653,\n",
    "  303244: 0.03312488833303554,\n",
    "  437665: 0.04622019308329931,\n",
    "  633092: 0.07368754094455364,\n",
    "  659771: 0.026289593915107587,\n",
    "  678530: 0.025553928902577575,\n",
    "  761990: 0.14351154614285722,\n",
    "  809822: 0.026151227631343872,\n",
    "  909389: 0.011830317261798414,\n",
    "  1028660: 0.03943439087266137,\n",
    "  1085751: 0.03505279188681011,\n",
    "  1164254: 0.05074450978677787,\n",
    "  1235905: 0.0528051860936959,\n",
    "  1254376: 0.01714828443740992,\n",
    "  1292566: 0.1134367920034917},\n",
    " '0.95_0.3': {1036559: 0.05183232436671778,\n",
    "  1067060: 0.009201357870287653,\n",
    "  1073172: 0.05578637537163887,\n",
    "  1089398: 0.024646494295413355,\n",
    "  120689: 0.023536104868209475,\n",
    "  1209066: 0.023290937109165626,\n",
    "  1212254: 0.0248438806336973,\n",
    "  1292718: 0.012651867071645525,\n",
    "  1294941: 0.053707250405481625,\n",
    "  339988: 0.02954658249459037,\n",
    "  348374: 0.009201357870287653,\n",
    "  352540: 0.006276541000925606,\n",
    "  374177: 0.01237785300561019,\n",
    "  393373: 0.11633884833057301,\n",
    "  397374: 0.013356809811707875,\n",
    "  437665: 0.03342011208408998,\n",
    "  43927: 0.057464876098353795,\n",
    "  464289: 0.01391649794235383,\n",
    "  535969: 0.015104705974537223,\n",
    "  59263: 0.027834107557620155,\n",
    "  61472: 0.019717195436330687,\n",
    "  639081: 0.021756037347406627,\n",
    "  641101: 0.031027314350447168,\n",
    "  722976: 0.019154654663308505,\n",
    "  761990: 0.14351154614285724,\n",
    "  805545: 0.009201357870287653,\n",
    "  854933: 0.009201357870287653,\n",
    "  899578: 0.012735322347293264,\n",
    "  940340: 0.020662698375382815,\n",
    "  944922: 0.026151227631343876,\n",
    "  984907: 0.03542176864351336,\n",
    "  'A': 0.019717195436330687,\n",
    "  'seq_23': 0.01291989223638457,\n",
    "  'seq_34': 0.004487547485919025},\n",
    " '0.98_0.1': {9541: 0.01035152760407361,\n",
    "  45658: 0.029738215203826315,\n",
    "  84904: 0.012816177033614951,\n",
    "  97268: 0.026188480092357164,\n",
    "  108028: 0.04925181091133321,\n",
    "  162020: 0.12808588780188707,\n",
    "  264555: 0.04690745311141012,\n",
    "  339787: 0.08088554754989338,\n",
    "  362546: 0.062166579989221496,\n",
    "  408215: 0.06022706969642828,\n",
    "  594089: 0.06937725947411036,\n",
    "  651227: 0.031054582812220836,\n",
    "  692572: 0.011501697337859568,\n",
    "  761990: 0.14351154614285724,\n",
    "  886644: 0.03858581364391068,\n",
    "  903269: 0.055139406657945284,\n",
    "  1032795: 0.02146983503067119,\n",
    "  1055332: 0.012076782204752548,\n",
    "  1118650: 0.09695250028708041,\n",
    "  1245471: 0.01371182741454632},\n",
    " '0.98_0.2': {4117: 0.04968733249955332,\n",
    "  6179: 0.030611798261165755,\n",
    "  20346: 0.013693359350270554,\n",
    "  77586: 0.013479400610422671,\n",
    "  137221: 0.06686668222761828,\n",
    "  256125: 0.10306616214468636,\n",
    "  279373: 0.03505279188681011,\n",
    "  437665: 0.04622019308329931,\n",
    "  633092: 0.08551785820635206,\n",
    "  676418: 0.025553928902577575,\n",
    "  761990: 0.14351154614285722,\n",
    "  803141: 0.03312488833303554,\n",
    "  940479: 0.018402715740575306,\n",
    "  944887: 0.009201357870287653,\n",
    "  997935: 0.01714828443740992,\n",
    "  1081371: 0.026151227631343872,\n",
    "  1164254: 0.05074450978677787,\n",
    "  1235905: 0.0528051860936959,\n",
    "  1253250: 0.026289593915107587,\n",
    "  1290853: 0.03943439087266137,\n",
    "  1292566: 0.1134367920034917},\n",
    " '0.98_0.3': {43927: 0.057464876098353795,\n",
    "  59263: 0.027834107557620155,\n",
    "  120689: 0.006901018402715741,\n",
    "  251297: 0.004495319875560348,\n",
    "  295057: 0.023536104868209475,\n",
    "  311283: 0.009201357870287653,\n",
    "  339988: 0.02954658249459037,\n",
    "  374177: 0.012735322347293264,\n",
    "  393373: 0.09977640416405523,\n",
    "  397374: 0.020662698375382815,\n",
    "  403716: 0.009201357870287653,\n",
    "  437665: 0.03342011208408998,\n",
    "  464289: 0.015104705974537223,\n",
    "  535969: 0.014492353029623685,\n",
    "  637033: 0.012554679477118974,\n",
    "  641101: 0.031027314350447168,\n",
    "  676121: 0.0167789467046422,\n",
    "  761990: 0.14954210053237535,\n",
    "  854933: 0.009201357870287653,\n",
    "  855052: 0.006901018402715741,\n",
    "  894893: 0.006395981466978193,\n",
    "  897721: 0.019717195436330687,\n",
    "  899578: 0.01237785300561019,\n",
    "  899769: 0.013075613815671938,\n",
    "  929185: 0.0230936040666043,\n",
    "  945094: 0.009201357870287653,\n",
    "  984907: 0.042322787046229104,\n",
    "  1028116: 0.006209237482540754,\n",
    "  1036559: 0.03875671055104585,\n",
    "  1073172: 0.05578637537163887,\n",
    "  1120458: 0.026308474047338718,\n",
    "  1167198: 0.026151227631343876,\n",
    "  1209066: 0.0315475126981291,\n",
    "  1212254: 0.026568365013999364,\n",
    "  1292632: 0.009201357870287653,\n",
    "  1292660: 0.009201357870287653,\n",
    "  1294941: 0.053707250405481625},\n",
    " '0.9_0.1': {6325: 0.021469835030671187,\n",
    "  24306: 0.012076782204752546,\n",
    "  45658: 0.029738215203826312,\n",
    "  108028: 0.0492518109113332,\n",
    "  162020: 0.12808588780188704,\n",
    "  264555: 0.04690745311141012,\n",
    "  319613: 0.03858581364391068,\n",
    "  332192: 0.01035152760407361,\n",
    "  339787: 0.08088554754989338,\n",
    "  362546: 0.06216657998922149,\n",
    "  408215: 0.06022706969642827,\n",
    "  594089: 0.06937725947411036,\n",
    "  630860: 0.013711827414546318,\n",
    "  753672: 0.031054582812220833,\n",
    "  761990: 0.14351154614285722,\n",
    "  817381: 0.02618848009235716,\n",
    "  903269: 0.05513940665794528,\n",
    "  1034438: 0.011501697337859568,\n",
    "  1118650: 0.0969525002870804,\n",
    "  1305066: 0.01281617703361495},\n",
    " '0.9_0.2': {4117: 0.04968733249955333,\n",
    "  6179: 0.030611798261165758,\n",
    "  16474: 0.017148284437409924,\n",
    "  31828: 0.013693359350270556,\n",
    "  137221: 0.06686668222761828,\n",
    "  159934: 0.013479400610422673,\n",
    "  256125: 0.10306616214468638,\n",
    "  274511: 0.02628959391510759,\n",
    "  437665: 0.04622019308329932,\n",
    "  594186: 0.03505279188681011,\n",
    "  633092: 0.07368754094455365,\n",
    "  678530: 0.02555392890257758,\n",
    "  761990: 0.14351154614285724,\n",
    "  782309: 0.011830317261798414,\n",
    "  803153: 0.03312488833303555,\n",
    "  809822: 0.026151227631343876,\n",
    "  813013: 0.018402715740575306,\n",
    "  1028781: 0.03943439087266137,\n",
    "  1075746: 0.009201357870287653,\n",
    "  1164254: 0.05074450978677787,\n",
    "  1235905: 0.05280518609369591,\n",
    "  1292566: 0.11343679200349172},\n",
    " '0.9_0.3': {1028116: 0.006209237482540754,\n",
    "  1036559: 0.03875671055104585,\n",
    "  10382: 0.019717195436330687,\n",
    "  1073172: 0.05578637537163887,\n",
    "  1073461: 0.009201357870287653,\n",
    "  1089398: 0.024646494295413355,\n",
    "  1120458: 0.0060305543895181465,\n",
    "  1167198: 0.031027314350447168,\n",
    "  1210206: 0.009201357870287653,\n",
    "  1212254: 0.02890674277996871,\n",
    "  1292746: 0.009201357870287653,\n",
    "  1294941: 0.053707250405481625,\n",
    "  295057: 0.026151227631343876,\n",
    "  298401: 0.012735322347293264,\n",
    "  339988: 0.02954658249459037,\n",
    "  365985: 0.013356809811707875,\n",
    "  374177: 0.012554679477118974,\n",
    "  393373: 0.09977640416405523,\n",
    "  397374: 0.012651867071645525,\n",
    "  437665: 0.03342011208408998,\n",
    "  43927: 0.043662839292922306,\n",
    "  456097: 0.013075613815671938,\n",
    "  464289: 0.01391649794235383,\n",
    "  535969: 0.014492353029623685,\n",
    "  589828: 0.016562444166517778,\n",
    "  59263: 0.027834107557620155,\n",
    "  61472: 0.006901018402715741,\n",
    "  637033: 0.01237785300561019,\n",
    "  641101: 0.023536104868209475,\n",
    "  676525: 0.006901018402715741,\n",
    "  721707: 0.006901018402715741,\n",
    "  722976: 0.019154654663308505,\n",
    "  761990: 0.14990752760983542,\n",
    "  884946: 0.019717195436330687,\n",
    "  894893: 0.006591278611008029,\n",
    "  940340: 0.020662698375382815,\n",
    "  942023: 0.009201357870287653,\n",
    "  945189: 0.006901018402715741,\n",
    "  984907: 0.03542176864351336,\n",
    "  985396: 0.009201357870287653,\n",
    "  'seq_46': 0.004495319875560348}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting better data-set: 9\n",
    "k = 9\n",
    "damping = dct_databases[k][0]\n",
    "threshold = dct_databases[k][1]\n",
    "x_train = dct_databases[k][2]\n",
    "Y_train = dct_databases[k][3]\n",
    "x_test = dct_databases[k][4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create \"First\" Bag Class (Decision tree) and test Single CV   \n",
    "\n",
    "2) Create \"Second\" Bag Class (SVC) and test Single CV <----\n",
    "\n",
    "3) Create Double (Model comparisson) Nested-CV\n",
    "\n",
    "4) Convert Single/Double Nested-CV into class\n",
    "\n",
    "5) Create a classifier inherates bag classes:  self.super()/decide prediction by votes\n",
    "\n",
    "6) Store classes as libraries"
   ]
  },
  {
   "source": [
    "## Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1) Bagging Trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingTrees:\n",
    "    \"\"\"Expand the subset of features in regards each node split for\n",
    "    a more flexible tunning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_t : np.array\n",
    "        Training set features.\n",
    "    Y_t : np.array\n",
    "        Training set labels.\n",
    "    X_v : np.array\n",
    "        Validation set features.\n",
    "    p : list\n",
    "            0: epochs\n",
    "            1: n_trees\n",
    "            2: criterion\n",
    "            3: min_samples_leaf\n",
    "            4: max_depth\n",
    "            5: min_samples_splits\n",
    "            6: max_leaf_nodes\n",
    "    Output\n",
    "    -------\n",
    "    y_pred:      predictions on validation set X_v (array)\n",
    "    unan_rates:  rate of majority votes (array)\n",
    "    acc:         accuracy on training set Y_t (integer)\n",
    "    f1:          f1 score on training set Y_t (integer)\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, p):\n",
    "\n",
    "        # store parameters   \n",
    "        self.epochs = p[0]; self.n_trees = p[1]\n",
    "        self.criterion = p[2]; self.min_samples_leaf = p[3]\n",
    "        self.max_depth = p[4]; self.min_samples_splits = p[5]\n",
    "        self.max_leaf_nodes = p[6]    \n",
    "        \n",
    "        \n",
    "    def fit(self, X_t, Y_t):\n",
    "        \n",
    "        if isinstance(X_t,np.ndarray):\n",
    "            X_t = pd.DataFrame(X_t)\n",
    "        elif not isinstance(X_t,pd.core.frame.DataFrame):\n",
    "            raise Exception('Wrong type for X_t. Expected np.ndarray or pd.DataFrame')\n",
    "\n",
    "        if isinstance(Y_t,np.ndarray):\n",
    "            Y_t = pd.DataFrame(Y_t)\n",
    "        elif not isinstance(X_v,pd.core.frame.DataFrame):\n",
    "            raise Exception('Wrong type for Y_t. Expected np.ndarray or pd.DataFrame')\n",
    "\n",
    "        self.X_t_df = X_t.copy(); self.Y_t_df = Y_t.copy()\n",
    "\n",
    "        X_t['label'] = Y_t\n",
    "        train_df = X_t\n",
    "        for i in range(self.epochs):\n",
    "            self.bag = []\n",
    "            for run in np.arange(self.n_trees):            \n",
    "                # resampling the dataframe (number of distinct, number of distinct)\n",
    "                train_df_bs = train_df.iloc[np.random.randint(len(train_df), size=len(train_df))]\n",
    "                X_train = train_df_bs.iloc[:,:-1]\n",
    "                Y_train = train_df_bs.iloc[:,-1:]\n",
    "                # Storing each trained tree\n",
    "                wl = DecisionTreeClassifier(criterion=self.criterion\n",
    "                                        , min_samples_leaf=self.min_samples_leaf\n",
    "                                        , max_depth=self.max_depth\n",
    "                                        , min_samples_split=self.min_samples_splits\n",
    "                                        , max_leaf_nodes=self.max_leaf_nodes).fit(X_train,Y_train) \n",
    "                                        #, random_state=run  \n",
    "                # add tree into bag\n",
    "                self.bag.append(wl)\n",
    "\n",
    "        ## Score on Training set\n",
    "        t_predictions = []\n",
    "        for i in range(self.n_trees):        \n",
    "            tree_t_prediction = self.bag[i].predict(self.X_t_df) # predict validation and training sets   \n",
    "            t_predictions.append(tree_t_prediction) # Append predictions\n",
    "        \n",
    "        # Convert predictions lists into np.array to transpose them and obtain \"n_tree\" predictions per line\n",
    "        t_predictions_T = np.array(t_predictions).T\n",
    "\n",
    "        t_final_predictions = []\n",
    "        # for each entry \"m\" of X_t_df(m x features)\n",
    "        for line in t_predictions_T:\n",
    "            # countabilize the \"n_tree\" votes in v_predictions_T (m x n_tree)\n",
    "            most_common = Counter(line).most_common(1)[0][0]\n",
    "            t_final_predictions.append(most_common) \n",
    "\n",
    "        # accuracies values\n",
    "        self.acc = accuracy_score(self.Y_t_df, t_final_predictions)\n",
    "        self.f1 = f1_score(self.Y_t_df, t_final_predictions, average='macro')\n",
    "        self.bcr = balanced_accuracy_score(self.Y_t_df, t_final_predictions)\n",
    "        self.auc = roc_auc_score(self.Y_t_df, t_final_predictions, average='macro')\n",
    "        return \n",
    "            \n",
    "    def predict(self, X_v): \n",
    "\n",
    "        if isinstance(X_v,np.ndarray):\n",
    "            X_v = pd.DataFrame(X_v)\n",
    "        elif not isinstance(X_v,pd.core.frame.DataFrame):\n",
    "            raise Exception('Wrong type for X_v. Expected np.ndarray or pd.DataFrame')\n",
    "\n",
    "        self.X_v_df = X_v.copy()\n",
    "        ## Prediction on Validation set\n",
    "        v_predictions = []\n",
    "        # each tree will make a prediction about test_df\n",
    "        for i in range(self.n_trees):\n",
    "            tree_v_prediction = self.bag[i].predict(self.X_v_df) # predict validation and training sets\n",
    "            v_predictions.append(tree_v_prediction) # Append predictions\n",
    "        # Convert predictions lists into np.array to transpose them and obtain \"n_tree\" predictions per line\n",
    "        v_predictions_T = np.array(v_predictions).T\n",
    "        \n",
    "        self.prediction = []   \n",
    "        self.votes = [] \n",
    "        # for each entry \"n\" of X_v_df(n x features)\n",
    "        for line in v_predictions_T:\n",
    "            # countabilize the \"n_tree\" votes in v_predictions_T (n x n_tree) \n",
    "            most_common = Counter(line).most_common(1)[0][0]\n",
    "            unanimity_rate = Counter(line)[most_common] / len(line)\n",
    "            # get prediction and unanimity rate\n",
    "            self.prediction.append(most_common)\n",
    "            self.votes.append(unanimity_rate)\n",
    "        return self.prediction\n",
    "        \n",
    "\n"
   ]
  },
  {
   "source": [
    "### 2) SVC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingSVC:\n",
    "    \"\"\"Expand the subset of features in regards each node split for\n",
    "    a more flexible tunning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_t : np.array\n",
    "        Training set features.\n",
    "    Y_t : np.array\n",
    "        Training set labels.\n",
    "    X_v : np.array\n",
    "        Validation set features.\n",
    "    p : list\n",
    "            0: epochs\n",
    "            1: n_svc\n",
    "            2: C\n",
    "            3: kernel\n",
    "            4: gamma\n",
    "            5: coef0\n",
    "            6: degree\n",
    "            7: tol\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "    y_pred:      predictions on validation set X_v (array)\n",
    "    unan_rates:  rate of majority votes (array)\n",
    "    acc:         accuracy on training set Y_t (integer)\n",
    "    f1:          f1 score on training set Y_t (integer)\n",
    "    \"\"\"\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "source": [
    "## Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 242)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m242\u001b[0m\n\u001b[1;33m    return results_frame\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# * FINISH SINGLE-MODEL NESTED AND THEM ADD THE DOUBLE-MODEL NESTE\n",
    "# ** AFTER MODIFIED THE BAGGIN INTO CLASS, MAKE CV-NESTED GENERICAL \n",
    "##########################################################################\n",
    "\n",
    "def nested_single_cv(x_t, y_t, k_ext, k_int, hp_set):\n",
    "    \"\"\"   \n",
    "    Help set a hyper-parameters list for a given model before makes \n",
    "    its comparison with others hyper-parameterized models.\n",
    "        \n",
    "    Input: \n",
    "        - x_t: features train (numpy.arrays)\n",
    "        - y_t: labels train (numpy.arrays)\n",
    "        - K_ext: number of external folds (integer) \n",
    "        - K_int: number of internal folds (integer)\n",
    "        - model: learning algorithm function (function or class)\n",
    "        - hp_set: list of parameters of the learning algorithm (array)\n",
    "\n",
    "    Output: \n",
    "        - train_acc mean\n",
    "        - train_acc std\n",
    "        - train_f1 mean\n",
    "        - train_f1 std        \n",
    "        - train_bcr mean\n",
    "        - train_bcr std\n",
    "        - test_acc mean\n",
    "        - test_acc std\n",
    "        - test_f1 mean\n",
    "        - test_f1 std\n",
    "        - test_bcr mean\n",
    "        - test_bcr std\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    results_frame = pd.DataFrame(columns =['hp_hat' \n",
    "                                        , 't_bcr_mean'\n",
    "                                        , 'v_bcr_mean'\n",
    "                                        , 'v_bcr_std'])\n",
    "    \n",
    "    # frame pointer\n",
    "    i = 0   \n",
    "    # partionate \"training rows\" into \"K_ext\" sets   \n",
    "    K_ext_folds = KFold(n_splits = k_ext, shuffle=False).split(x_t)  # (markers t_i, v_i)\n",
    "    for t_ext_fold, v_ext_fold in K_ext_folds:\n",
    "        # sectioning \"train set\" between \"S_k\" into \"ext_fold\" sets            \n",
    "        x_S_k = x_t[t_ext_fold] # training x\n",
    "        y_S_k = y_t[t_ext_fold] # training y            \n",
    "        x_ext_fold = x_t[v_ext_fold] # test x \n",
    "        y_ext_fold = y_t[v_ext_fold] # test y\n",
    "        \n",
    "        \n",
    "        # get hp_hat in the inner loop\n",
    "        hp_hat, hp_mean = None, 0\n",
    "        hp_dic = {}\n",
    "        for idx, hp in enumerate(hp_set):  \n",
    "            hp_dic[idx]=[]\n",
    "            # partionate \"S_k training rows\" into \"K_int\" sets\n",
    "            K_int_folds = KFold(n_splits = k_int, shuffle=False).split(x_S_k)   \n",
    "            for t_int_fold, v_int_fold in K_int_folds:\n",
    "                # sectioning \"S_k\" between \"Ss_k\" into \"int_fold\" sets                \n",
    "                x_Ss_k = x_S_k[t_int_fold] # training x\n",
    "                y_Ss_k = y_S_k[t_int_fold] # training y  \n",
    "                x_int_fold = x_S_k[v_int_fold] # test x\n",
    "                y_int_fold = y_S_k[v_int_fold] # test y\n",
    "                \n",
    "                # must scaler after partition, for specific a training normalization\n",
    "                min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                X_t = min_max_scaler.fit_transform(x_Ss_k)\n",
    "                X_v = min_max_scaler.fit_transform(x_int_fold)\n",
    "                Y_t = y_Ss_k\n",
    "                Y_v = y_int_fold\n",
    "                \n",
    "                # Loading models\n",
    "                m = bagging_trees(X_t, Y_t, X_v, hp)\n",
    "                # prediction\n",
    "                Y_v_predicted = m[0]             \n",
    "                # validation\n",
    "                v_bcr = balanced_accuracy_score(Y_v, Y_v_predicted)\n",
    "                # append all \n",
    "                hp_dic[idx].append(v_bcr)\n",
    "            \n",
    "        # avg all hp predictions scores and define the higher to hp_hat\n",
    "        ixd_max= max([(k,np.mean(v)) for k,v in hp_dic.items()],key=lambda item:item[1])[0]\n",
    "        hp_hat = hp_set[ixd_max]  \n",
    "        \n",
    "        # must scaler after partition, for specific a training normalization\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_t = min_max_scaler.fit_transform(x_S_k)\n",
    "        X_v = min_max_scaler.fit_transform(x_ext_fold)\n",
    "        Y_t = y_S_k\n",
    "        Y_v = y_ext_fold\n",
    "\n",
    "        #  Loading models                       \n",
    "        m = bagging_trees(X_t, Y_t, X_v, hp_hat) # v_final_predictions, unanimity_rates, acc, bcr, f1\n",
    "\n",
    "        # validation\n",
    "        Y_v_predicted = m[0]\n",
    "\n",
    "        # training\n",
    "        t_acc = m[2]\n",
    "        t_bcr = m[3]\n",
    "        t_f1 = m[4] \n",
    "        t_auc = m[5] \n",
    "        # validation\n",
    "        v_acc = accuracy_score(Y_v, Y_v_predicted)\n",
    "        v_bcr = balanced_accuracy_score(Y_v, Y_v_predicted)\n",
    "        v_f1 = f1_score(Y_v, Y_v_predicted, average='macro')\n",
    "        v_auc = roc_auc_score(Y_v, Y_v_predicted, average='macro')\n",
    "    \n",
    "        results_frame.loc[i] = [hp_hat \n",
    "                                , t_bcr_mean\n",
    "                                , v_bcr_mean\n",
    "                                , v_bcr_std]\n",
    "        i += 1\n",
    "    \n",
    "        \n",
    "    return results_frame\n",
    "\n",
    "def nested_single_cv2(x_t, y_t, L, k_ext, k_int, hp_set):\n",
    "    \"\"\"   \n",
    "    Help set a hyper-parameters list for a given model before makes \n",
    "    its comparison with others hyper-parameterized models.\n",
    "        \n",
    "    Input: \n",
    "        - x_t: features train (numpy.arrays)\n",
    "        - y_t: labels train (numpy.arrays)\n",
    "        - K_ext: number of external folds (integer) \n",
    "        - K_int: number of internal folds (integer)\n",
    "        - model: learning algorithm function (function or class)\n",
    "        - hp_set: list of parameters of the learning algorithm (array)\n",
    "\n",
    "    Output: \n",
    "        - train_acc mean\n",
    "        - train_acc std\n",
    "        - train_f1 mean\n",
    "        - train_f1 std        \n",
    "        - train_bcr mean\n",
    "        - train_bcr std\n",
    "        - test_acc mean\n",
    "        - test_acc std\n",
    "        - test_f1 mean\n",
    "        - test_f1 std\n",
    "        - test_bcr mean\n",
    "        - test_bcr std\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    results_frame = pd.DataFrame(columns =['hp_hat' \n",
    "                                        , 't_acc'\n",
    "                                        , 'v_acc'\n",
    "                                        , 't_bcr'\n",
    "                                        , 'v_bcr'\n",
    "                                        , 't_f1'\n",
    "                                        , 'v_f1'\n",
    "                                        , 't_auc'\n",
    "                                        , 'v_auc'])\n",
    "    \n",
    "    # frame pointer\n",
    "    i = 0\n",
    "    # experiemental 3 metrics each one with mean & std \n",
    "    outer = np.empty((0,6), float)\n",
    "    # partionate \"training rows\" into \"K_ext\" sets   \n",
    "    K_ext_folds = KFold(n_splits = k_ext, shuffle=False).split(x_t)  # (markers t_i, v_i)\n",
    "    for t_ext_fold, v_ext_fold in K_ext_folds:\n",
    "        # sectioning \"train set\" between \"S_k\" into \"ext_fold\" sets            \n",
    "        x_S_k = x_t[t_ext_fold] # training x\n",
    "        y_S_k = y_t[t_ext_fold] # training y            \n",
    "        x_ext_fold = x_t[v_ext_fold] # test x \n",
    "        y_ext_fold = y_t[v_ext_fold] # test y\n",
    "        \n",
    "        \n",
    "        # get hp_hat in the inner loop\n",
    "        hp_hat, hp_mean = None, 0\n",
    "        hp_dic = {}\n",
    "        for idx, hp in enumerate(hp_set):  \n",
    "            hp_dic[idx]=[]\n",
    "            # partionate \"S_k training rows\" into \"K_int\" sets\n",
    "            K_int_folds = KFold(n_splits = k_int, shuffle=False).split(x_S_k)   \n",
    "            for t_int_fold, v_int_fold in K_int_folds:\n",
    "                # sectioning \"S_k\" between \"Ss_k\" into \"int_fold\" sets                \n",
    "                x_Ss_k = x_S_k[t_int_fold] # training x\n",
    "                y_Ss_k = y_S_k[t_int_fold] # training y  \n",
    "                x_int_fold = x_S_k[v_int_fold] # test x\n",
    "                y_int_fold = y_S_k[v_int_fold] # test y\n",
    "                \n",
    "                # must scaler after partition, for specific a training normalization\n",
    "                min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                X_t = min_max_scaler.fit_transform(x_Ss_k)\n",
    "                X_v = min_max_scaler.fit_transform(x_int_fold)\n",
    "                Y_t = y_Ss_k\n",
    "                Y_v = y_int_fold\n",
    "                \n",
    "                # Loading and fitting model\n",
    "                model = L(hp)\n",
    "                model.fit(X_t, Y_t)\n",
    "                # prediction\n",
    "                Y_v_predicted = model.predict(X_v)             \n",
    "                # validation\n",
    "                v_bcr = balanced_accuracy_score(Y_v, Y_v_predicted)\n",
    "                # append all \n",
    "                hp_dic[idx].append(v_bcr)\n",
    "            \n",
    "        # avg all hp predictions scores and define the higher to hp_hat\n",
    "        ixd_max= max([(k,np.mean(v)) for k,v in hp_dic.items()],key=lambda item:item[1])[0]\n",
    "        hp_hat = hp_set[ixd_max]  \n",
    "        \n",
    "        # must scaler after partition, for specific a training normalization\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_t = min_max_scaler.fit_transform(x_S_k)\n",
    "        X_v = min_max_scaler.fit_transform(x_ext_fold)\n",
    "        Y_t = y_S_k\n",
    "        Y_v = y_ext_fold\n",
    "\n",
    "        # Loading and fitting model\n",
    "        model = L(hp)\n",
    "        model.fit(X_t, Y_t)\n",
    "        # prediction\n",
    "        Y_v_predicted = model.predict(X_v)   \n",
    "\n",
    "        # training\n",
    "        t_acc = model.acc\n",
    "        t_bcr = model.bcr\n",
    "        t_f1 = model.f1\n",
    "        t_auc = model.auc \n",
    "        # validation\n",
    "        v_acc = accuracy_score(Y_v, Y_v_predicted)\n",
    "        v_bcr = balanced_accuracy_score(Y_v, Y_v_predicted)\n",
    "        v_f1 = f1_score(Y_v, Y_v_predicted, average='macro')\n",
    "        v_auc = roc_auc_score(Y_v, Y_v_predicted, average='macro')\n",
    "        \n",
    "        results_frame.loc[i] = [hp_hat \n",
    "                                , t_acc\n",
    "                                , v_acc\n",
    "                                , t_bcr\n",
    "                                , v_bcr\n",
    "                                , t_f1\n",
    "                                , v_f1\n",
    "                                , t_auc\n",
    "                                , v_auc]\n",
    "        i += 1\n",
    "\n",
    "return results_frame\n"
   ]
  },
  {
   "source": [
    "## Tunning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4ae8093af270>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# import...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Y_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# OPERATE SINGEL AND DOUBLE NESTED LOOPS FOR MULTIPLE MODEL AND PARAMETERS\n",
    "# * Plot distribution graphs\n",
    "##########################################################################\n",
    "\n",
    "# import...\n",
    "x_train = pd.read_csv('x_train.csv')\n",
    "Y_train = pd.read_csv('Y_train.csv')\n",
    "x_test = pd.read_csv('x_test.csv')\n",
    "\n",
    "# convert o numpy\n",
    "x_t = x_train.values\n",
    "y_t = Y_train.values\n",
    "x_v = x_test.values\n",
    "\n",
    "# Model1: Bagging_trees\n",
    "grid = {'epochs':[1]\n",
    "                    , 'n_trees':[10]\n",
    "                    , 'criterion': ['entropy']\n",
    "                    , 'min_samples_leaf':[0.06]\n",
    "                    , 'max_depth':[2, 3]\n",
    "                    , 'min_samples_split':[0.05]\n",
    "                    , 'max_leaf_nodes':[200]\n",
    "                    }\n",
    "model1= BaggingTrees\n",
    "hp_set1 = [v for v in product(*grid.values())]\n",
    "\n",
    "\n",
    "# nested-cv:,comp. 67% : (criterion = 'entropy', min_samples_leaf = 0.06, max_depth = 3, min_samples_split = 0.02, max_leaf_nodes = 200)\n",
    "# nested-cv:75%(run), comp. 67% : (criterion = 'entropy', min_samples_leaf = 0.06, max_depth = 3, min_samples_split = 0.02, max_leaf_nodes = 200)\n",
    "# nested-cv: 74%(run), comp. ? :(criterion = 'entropy', min_samples_leaf = 0.1, max_depth = 10, min_samples_split = 0.03, max_leaf_nodes = 200)\n",
    "\n",
    "\n",
    "# Model2: SVC\n",
    "grid = {'C': np.arange( 1, 50+1, 10).tolist()\n",
    "                    , 'kernel': ['rbf', 'sigmoid', 'poly']\n",
    "                    , 'gamma':  np.arange( 0, 3, 1).tolist()\n",
    "                    , 'coef0': np.arange( 0, 10, 2 ).tolist()        \n",
    "                    , 'degree': np.arange( 0, 100+0, 1 ).tolist()\n",
    "                    , 'tol': np.arange( 0.001, 0.01, 0.003 ).tolist()\n",
    "                    #, 'shrinking': [True]\n",
    "                    #, 'probability': [False] \n",
    "                    #, 'cache_size':   [2000]\n",
    "                    #, 'class_weight': [None]\n",
    "                    #, 'verbose':      [False]\n",
    "                    #, 'max_iter':     [-1]\n",
    "                    #, 'random_state': [None]\n",
    "                    }\n",
    "\n",
    "\n",
    "# model2= SVC()\n",
    "hp_set2 = [v for v in product(*grid.values())]\n",
    "\n",
    "\n",
    "# run validation...\n",
    "\n",
    "#########################\n",
    "# ... single model nested\n",
    "#########################\n",
    "\n",
    "\n",
    "# to analyse the results of a single cross-validation\n",
    "\n",
    "K_int, K_ext = 5, 5\n",
    "# L = bagging_trees() ->  missing 4 required positional arguments: 'X_t', 'Y_t', 'X_v', and 'p'\n",
    "\n",
    "# only training set\n",
    "#results = nested_single_cv(x_t, y_t, K_ext, K_int, hp_set1)\n",
    "results2 = nested_single_cv2(x_t, y_t, model1, K_ext, K_int, hp_set1)\n",
    "    \n",
    "    \n",
    "# #########################    \n",
    "# # ... single model nested -> *high computational cost, sent maximum 2 HP per model*\n",
    "# #########################    \n",
    "\n",
    "# K_int, K_out = 10, 10\n",
    "    \n",
    "# results = nested_cv_loops(X_train, Y_train, K_int, K_out, L1, L2, HP1, HP2)\n",
    "# ????\n",
    "results.groupby('hp_hat').agg({'t_acc': ['count', 'mean', 'std']\n",
    "                              , 'v_acc': ['mean', 'std']\n",
    "                              , 't_bcr': ['mean', 'std']\n",
    "                              , 'v_bcr': ['mean', 'std']\n",
    "                              , 't_f1': ['mean', 'std']\n",
    "                              , 'v_f1': ['mean', 'std']\n",
    "                              , 't_auc': ['mean', 'std']\n",
    "                              , 'v_auc': ['mean', 'std']\n",
    "                              }).reset_index('hp_hat')"
   ]
  },
  {
   "source": [
    "## Figuring out my submission"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[0.85, 0.84, 0.95, 0.97, 0.98, 0.73, 0.99, 0.97, 0.92, 0.86, 0.91, 0.77, 0.82, 0.77, 0.78, 0.83, 0.92, 1.0, 0.81, 0.81, 1.0, 0.91, 0.73, 0.88, 0.53, 0.88, 0.87, 1.0, 0.97, 0.99, 0.95, 0.74, 1.0, 0.99, 0.77, 0.81, 0.88, 0.7, 0.83, 0.52, 0.99, 0.95, 0.81, 0.98, 0.98, 0.99, 0.71, 0.78, 0.97, 0.98, 0.53, 1.0, 0.75, 0.77, 0.64, 0.8, 0.86, 0.58, 0.87, 0.68, 0.93, 0.81, 0.74, 0.97, 0.86, 0.77, 0.99, 0.68, 1.0, 0.86, 1.0, 0.99, 0.76, 0.59, 0.96, 0.7, 0.96, 0.8, 0.82, 0.97, 0.78, 0.52, 1.0, 0.98, 0.94, 0.54, 0.99, 0.55, 0.79, 0.75, 0.77, 0.65, 1.0, 0.7, 0.98, 0.99, 0.89, 0.51, 0.99, 0.9, 0.92, 0.99, 1.0]\n",
      "[1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "votes [0.85, 0.84, 0.93, 0.98, 0.93, 0.8, 1.0, 0.99, 0.94, 0.87, 0.95, 0.87, 0.92, 0.71, 0.89, 0.85, 0.96, 0.98, 0.87, 0.88, 1.0, 0.94, 0.82, 0.95, 0.51, 0.9, 0.84, 1.0, 0.9, 0.95, 0.9, 0.79, 1.0, 0.99, 0.76, 0.88, 0.88, 0.67, 0.84, 0.57, 0.96, 0.95, 0.9, 1.0, 0.98, 0.94, 0.51, 0.86, 0.99, 0.92, 0.54, 1.0, 0.74, 0.89, 0.61, 0.78, 0.82, 0.55, 0.91, 0.66, 0.95, 0.85, 0.72, 0.95, 0.82, 0.75, 0.99, 0.63, 0.99, 0.95, 0.94, 0.98, 0.7, 0.58, 0.94, 0.71, 0.91, 0.77, 0.86, 1.0, 0.77, 0.54, 1.0, 1.0, 0.93, 0.54, 1.0, 0.51, 0.91, 0.9, 0.91, 0.65, 0.96, 0.81, 0.98, 1.0, 0.74, 0.53, 0.92, 0.91, 0.95, 0.99, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# import...\n",
    "x_train = pd.read_csv('x_train.csv')\n",
    "y_t = pd.read_csv('Y_train.csv')\n",
    "x_test = pd.read_csv('x_test.csv')\n",
    "\n",
    "\n",
    "# convert o numpy\n",
    "x_t = x_train.values\n",
    "y_t = Y_train.values\n",
    "x_v = x_test.values\n",
    "\n",
    "\n",
    "# normalize (np.ndarray)\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_t = min_max_scaler.fit_transform(x_t)\n",
    "x_v = min_max_scaler.fit_transform(x_v)\n",
    "\n",
    "# parameters\n",
    "p0 = [1, 100, 'entropy', 0.06, 3, 0.02, 200] # submission\n",
    "p1 = [1, 100, \"entropy\", 0.01, 2, 0.05,\t200]\n",
    "\n",
    "#results67 = bagging_trees_67(x_t, y_t, x_v)\n",
    "results = bagging_trees(x_t, y_t, x_v, p0)\n",
    "model = BaggingTrees(p0)\n",
    "model.fit(x_t, y_t)\n",
    "\n",
    "#print(results67)\n",
    "print(results[0])\n",
    "print(results[1])\n",
    "print(model.predict(x_v))\n",
    "print('votes',model.votes)\n",
    "# print('acc', results[2])\n",
    "# print('bcr', results[3])\n",
    "# print('f1', results[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_trees_67(X_t, Y_t, X_v):\n",
    "        \n",
    "    # train_df \n",
    "    train_df = pd.DataFrame(X_t).copy()\n",
    "    train_df['label'] = pd.DataFrame(Y_t)\n",
    "    \n",
    "    ## test_df    \n",
    "    test_df = pd.DataFrame(X_v).copy()\n",
    "\n",
    "    n_tree = 100\n",
    "    \n",
    "\n",
    "    bag = []\n",
    "    for run in np.arange(n_tree):\n",
    "        ## resampling the dataframe (size=400) 100 times\n",
    "        train_df_bs = train_df.iloc[np.random.randint(len(train_df), size=len(train_df))]\n",
    "        X_train = train_df_bs.iloc[:,:-1]\n",
    "        Y_train = train_df_bs.iloc[:,-1:]\n",
    "        \n",
    "        ## Storing each trained tree\n",
    "        wl = DecisionTreeClassifier(criterion = 'entropy'\n",
    "                                        , min_samples_leaf = 0.06\n",
    "                                        , max_depth = 3\n",
    "                                        , min_samples_split = 0.02\n",
    "                                        , max_leaf_nodes = 200).fit(X_train,Y_train)\n",
    "        bag.append(wl)\n",
    "\n",
    "    X_test = test_df\n",
    "\n",
    "    ## storing prediction for each line of X_test for each bootstrapping trainned tree  106x100\n",
    "    predictions = []\n",
    "    for i in range(n_tree):\n",
    "        predict = bag[i].predict(X_test)\n",
    "        predictions.append(predict)\n",
    "\n",
    "    # Transposing: each line predictions (for each bootstrapping trainned tree)\n",
    "    predictions= np.array(predictions)\n",
    "    prediction_by_row = predictions.T\n",
    "\n",
    "    # Counting the 100 votes\n",
    "    most_common_predictions = []\n",
    "    for line in prediction_by_row:\n",
    "        most_common = Counter(line).most_common(1)[0][0]\n",
    "        most_common_predictions.append(most_common)\n",
    "        \n",
    "    Y_v_predicted = most_common_predictions\n",
    "        \n",
    "    return Y_v_predicted\n",
    "\n",
    "def bagging_trees(X_t, Y_t, X_v, p):\n",
    "\n",
    "    # # train_df \n",
    "    # train_df = pd.DataFrame(X_t).copy()\n",
    "    # train_df['label'] = pd.DataFrame(Y_t)\n",
    "    \n",
    "    # ## test_df    \n",
    "    # test_df = pd.DataFrame(X_v).copy()\n",
    "\n",
    "    # n_tree = 100\n",
    "    \n",
    "\n",
    "    # bag = []\n",
    "    # for run in np.arange(n_tree):\n",
    "    #     ## resampling the dataframe (size=400) 100 times\n",
    "    #     train_df_bs = train_df.iloc[np.random.randint(len(train_df), size=len(train_df))]\n",
    "    #     X_train = train_df_bs.iloc[:,:-1]\n",
    "    #     Y_train = train_df_bs.iloc[:,-1:]\n",
    "        \n",
    "    #     ## Storing each trained tree\n",
    "    #     wl = DecisionTreeClassifier(criterion = 'entropy'\n",
    "    #                                     , min_samples_leaf = 0.06\n",
    "    #                                     , max_depth = 3\n",
    "    #                                     , min_samples_split = 0.02\n",
    "    #                                     , max_leaf_nodes = 200).fit(X_train,Y_train)\n",
    "    #     bag.append(wl)\n",
    "\n",
    "    # X_test = test_df\n",
    "\n",
    "    # ## storing prediction for each line of X_test for each bootstrapping trainned tree  106x100\n",
    "    # predictions = []\n",
    "    # for i in range(n_tree):\n",
    "    #     predict = bag[i].predict(X_test)\n",
    "    #     predictions.append(predict)\n",
    "\n",
    "    # # Transposing: each line predictions (for each bootstrapping trainned tree)\n",
    "    # predictions= np.array(predictions)\n",
    "    # prediction_by_row = predictions.T\n",
    "\n",
    "    # # Counting the 100 votes\n",
    "    # most_common_predictions = []\n",
    "    # for line in prediction_by_row:\n",
    "    #     most_common = Counter(line).most_common(1)[0][0]\n",
    "    #     most_common_predictions.append(most_common)\n",
    "        \n",
    "    # Y_v_predicted = most_common_predictions\n",
    "        \n",
    "    # return Y_v_predicted\n",
    "        \n",
    "    \n",
    "    #  make \"X_t\" and \"Y_t\" a pd.Dataframe\n",
    "    X_t_df = pd.DataFrame(X_t)\n",
    "    Y_t_df = pd.DataFrame(Y_t)\n",
    "        \n",
    "    # make \"X_v\" a pd.Dataframe    \n",
    "    X_v_df = pd.DataFrame(X_v).copy()    \n",
    "\n",
    "    # make \"train_df\" to resample \n",
    "    train_df = X_t_df.copy() \n",
    "    train_df['label'] = Y_t_df\n",
    "\n",
    "\n",
    "\n",
    "    bag = []\n",
    "    for run in np.arange(p[1]):            \n",
    "        # resampling the dataframe (number of distinct, number of distinct)\n",
    "        train_df_bs = train_df.iloc[np.random.randint(len(train_df), size=len(train_df))]\n",
    "        X_train = train_df_bs.iloc[:,:-1]\n",
    "        Y_train = train_df_bs.iloc[:,-1:]\n",
    "\n",
    "        # Storing each trained tree\n",
    "\n",
    "        wl = DecisionTreeClassifier(criterion=p[2]\n",
    "                                    , min_samples_leaf=p[3]\n",
    "                                    , max_depth=p[4]\n",
    "                                    , min_samples_split=p[5]\n",
    "                                    , max_leaf_nodes=p[6]).fit(X_train,Y_train) \n",
    "                                    #, random_state=run  \n",
    "        # add tree into bag\n",
    "        bag.append(wl)\n",
    "\n",
    "       \n",
    "    v_predictions = []\n",
    "    t_predictions = []\n",
    "\n",
    "    # each tree will make a prediction about test_df\n",
    "    for i in range(p[1]):\n",
    "        \n",
    "        # predict validation and training sets\n",
    "        tree_v_prediction = bag[i].predict(X_v_df)\n",
    "        tree_t_prediction = bag[i].predict(X_t_df)\n",
    "        \n",
    "        # Append predictions\n",
    "        v_predictions.append(tree_v_prediction)\n",
    "        t_predictions.append(tree_t_prediction)\n",
    "\n",
    "    # Convert predictions lists into np.array to transpose them and obtain \"n_tree\" predictions per line\n",
    "    v_predictions_T = np.array(v_predictions).T\n",
    "    t_predictions_T = np.array(t_predictions).T\n",
    "\n",
    "    ## Score on Training set\n",
    "    t_final_predictions = []\n",
    "    # for each entry \"m\" of X_t_df(m x features)\n",
    "    for line in t_predictions_T:\n",
    "        # countabilize the \"n_tree\" votes in v_predictions_T (m x n_tree)\n",
    "        most_common = Counter(line).most_common(1)[0][0]\n",
    "        t_final_predictions.append(most_common)  \n",
    "        \n",
    "    # accuracy and f1\n",
    "    acc = accuracy_score(Y_t_df, t_final_predictions)\n",
    "    f1 = f1_score(Y_t_df, t_final_predictions, average='macro')\n",
    "    bcr = balanced_accuracy_score(Y_t_df, t_final_predictions)\n",
    "    auc = roc_auc_score(Y_t_df, t_final_predictions, average='macro')\n",
    "        \n",
    "    ## Prediction on Validation set\n",
    "    v_final_predictions = []   \n",
    "    unanimity_rates = [] \n",
    "    # for each entry \"n\" of X_v_df(n x features)\n",
    "    for line in v_predictions_T:\n",
    "        # countabilize the \"n_tree\" votes in v_predictions_T (n x n_tree) \n",
    "        most_common = Counter(line).most_common(1)[0][0]\n",
    "        unanimity_rate = Counter(line)[most_common] / len(line)\n",
    "        # get prediction and unanimity rate\n",
    "        v_final_predictions.append(most_common)\n",
    "        unanimity_rates.append(unanimity_rate)\n",
    "        \n",
    "\n",
    "    return v_final_predictions , unanimity_rates, acc, bcr, f1, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Submitted prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damping</th>\n",
       "      <th>thres</th>\n",
       "      <th>TrainAcc_mean</th>\n",
       "      <th>TrainAcc_std</th>\n",
       "      <th>TestAcc_mean</th>\n",
       "      <th>TestAcc_std</th>\n",
       "      <th>BCR_mean</th>\n",
       "      <th>BCR_std</th>\n",
       "      <th>criterion</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.789657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   damping  thres  TrainAcc_mean  TrainAcc_std  TestAcc_mean  TestAcc_std  \\\n",
       "0      0.9    0.1       0.789657           0.0       0.75743          0.0   \n",
       "\n",
       "   BCR_mean  BCR_std criterion  min_samples_leaf max_depth  min_samples_split  \\\n",
       "0  0.740518      0.0   entropy              0.01         2               0.05   \n",
       "\n",
       "  max_leaf_nodes  \n",
       "0            200  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree # k-fold5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damping</th>\n",
       "      <th>thres</th>\n",
       "      <th>TrainAcc_mean</th>\n",
       "      <th>TrainAcc_std</th>\n",
       "      <th>TestAcc_mean</th>\n",
       "      <th>TestAcc_std</th>\n",
       "      <th>BCR_mean</th>\n",
       "      <th>BCR_std</th>\n",
       "      <th>criterion</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.821193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.731253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   damping  thres  TrainAcc_mean  TrainAcc_std  TestAcc_mean  TestAcc_std  \\\n",
       "0      0.9    0.1       0.821193           0.0      0.731253          0.0   \n",
       "\n",
       "   BCR_mean  BCR_std criterion  min_samples_leaf max_depth  min_samples_split  \\\n",
       "0  0.717038      0.0   entropy              0.01         2               0.05   \n",
       "\n",
       "  max_leaf_nodes  \n",
       "0            200  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag # k-fold5, 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damping</th>\n",
       "      <th>thres</th>\n",
       "      <th>TrainAcc_mean</th>\n",
       "      <th>TrainAcc_std</th>\n",
       "      <th>TestAcc_mean</th>\n",
       "      <th>TestAcc_std</th>\n",
       "      <th>BCR_mean</th>\n",
       "      <th>BCR_std</th>\n",
       "      <th>criterion</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.797195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.702214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.677592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   damping  thres  TrainAcc_mean  TrainAcc_std  TestAcc_mean  TestAcc_std  \\\n",
       "0      0.9    0.1       0.797195           0.0      0.702214          0.0   \n",
       "\n",
       "   BCR_mean  BCR_std criterion  min_samples_leaf max_depth  min_samples_split  \\\n",
       "0  0.677592      0.0   entropy              0.01         2               0.05   \n",
       "\n",
       "  max_leaf_nodes  \n",
       "0            200  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree # k-fold4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damping</th>\n",
       "      <th>thres</th>\n",
       "      <th>n_tree</th>\n",
       "      <th>TrainAcc_mean</th>\n",
       "      <th>TrainAcc_std</th>\n",
       "      <th>TestAcc_mean</th>\n",
       "      <th>TestAcc_std</th>\n",
       "      <th>BCR_mean</th>\n",
       "      <th>BCR_std</th>\n",
       "      <th>criterion</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.862446</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.772467</td>\n",
       "      <td>0.060262</td>\n",
       "      <td>0.783448</td>\n",
       "      <td>0.069478</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   damping  thres n_tree  TrainAcc_mean  TrainAcc_std  TestAcc_mean  \\\n",
       "0      0.9    0.1    100       0.862446      0.015038      0.772467   \n",
       "\n",
       "   TestAcc_std  BCR_mean   BCR_std criterion  min_samples_leaf max_depth  \\\n",
       "0     0.060262  0.783448  0.069478   entropy              0.06         3   \n",
       "\n",
       "   min_samples_split max_leaf_nodes  \n",
       "0               0.02            200  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag # k-fold4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x288483306a0>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sc\n",
    "\n",
    "\n",
    "\n",
    "x = np.linspace(0.01,1, 100)\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "plt.plot(x, sc.norm.pdf(x,loc=0.859214,scale=0.011093),\n",
    "        'k-', lw=2, alpha=0.6, label='Training', color='r')\n",
    "\n",
    "plt.plot(x, sc.norm.pdf(x,loc=0.757231,scale=0.044297),\n",
    "'k-', lw=2, alpha=0.6, label='Validation', color='b')\n",
    "\n",
    "plt.plot(x, sc.norm.pdf(x,loc=0.763987,scale=0.04823),\n",
    "'k-', lw=2, alpha=0.6, label='BCR', color='g')\n",
    "\n",
    "plt.legend([\"pdf of T_n-1\", \"t_obs\", \"t_a\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Competition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "a4c97918448b2f30896c96ac6fcc6e0acbe53b489c7c4eda4899ab81d4beed22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}