{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/Breno-st/ds-utils.git\n",
      "  Cloning https://github.com/Breno-st/ds-utils.git to c:\\users\\b_tib\\appdata\\local\\temp\\pip-req-build-c6qfe2m3\n",
      "Requirement already satisfied (use --upgrade to upgrade): ensemble==0.1 from git+https://github.com/Breno-st/ds-utils.git in c:\\users\\b_tib\\anaconda3\\lib\\site-packages\n",
      "Building wheels for collected packages: ensemble\n",
      "  Building wheel for ensemble (setup.py): started\n",
      "  Building wheel for ensemble (setup.py): finished with status 'done'\n",
      "  Created wheel for ensemble: filename=ensemble-0.1-py3-none-any.whl size=1056 sha256=3b67d34af925f710035b0d28325a304bdbaf76ee018a786a4c05ddf02df87f7c\n",
      "  Stored in directory: C:\\Users\\b_tib\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-d1rd4h5v\\wheels\\0e\\fb\\8a\\9c1e552752417e45ef056479c772e6b04146550c654c3f9e17\n",
      "Successfully built ensemble\n",
      "  Running command git clone -q https://github.com/Breno-st/ds-utils.git 'C:\\Users\\b_tib\\AppData\\Local\\Temp\\pip-req-build-c6qfe2m3'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/Breno-st/ds-utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+http://github.com/brendanhasz/dsutils.git  Running command git clone -q http://github.com/brendanhasz/dsutils.git 'C:\\Users\\b_tib\\AppData\\Local\\Temp\\pip-req-build-ogoo5wbb'\n",
      "\n",
      "  Cloning http://github.com/brendanhasz/dsutils.git to c:\\users\\b_tib\\appdata\\local\\temp\\pip-req-build-ogoo5wbb\n",
      "Requirement already satisfied (use --upgrade to upgrade): dsutils==0.1 from git+http://github.com/brendanhasz/dsutils.git in c:\\users\\b_tib\\anaconda3\\lib\\site-packages\n",
      "Building wheels for collected packages: dsutils\n",
      "  Building wheel for dsutils (setup.py): started\n",
      "  Building wheel for dsutils (setup.py): finished with status 'done'\n",
      "  Created wheel for dsutils: filename=dsutils-0.1-py3-none-any.whl size=44066 sha256=7216cb6b557de5020fc5b8eeaedacd1e9efbb8c430c44f0ff25ec0439a52bdd0\n",
      "  Stored in directory: C:\\Users\\b_tib\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-mggm7j4i\\wheels\\ab\\b0\\69\\592ce66f02b9cc72e3e0419119d5688e56ce89adb60ea7ce13\n",
      "Successfully built dsutils\n"
     ]
    }
   ],
   "source": [
    "!pip install git+http://github.com/brendanhasz/dsutils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ensemble'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-595d5944dcf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#from dsutils.optimization import optimize_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbootstrapped_aggregation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaggingTrees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ensemble'"
     ]
    }
   ],
   "source": [
    "from dsutils.optimization import optimize_params\n",
    "from ensemble.bootstrapped_aggregation import BaggingTrees"
   ]
  },
  {
   "source": [
    "## Importing libraries..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NzTRBPwN8KPX"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "import collections \n",
    "from collections import Counter\n",
    "\n",
    "# models\n",
    "from sklearn import  svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Feature selection\n",
    "from sklearn import feature_selection\n",
    "from sklearn import preprocessing,  svm\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# validation\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Some function to help on data cleaning/processing..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUXILIARY FUNCTIONS\n",
    "\n",
    "\n",
    "## FOR CATEGORIES:\n",
    "def alpha_views(dataframe, views_ref=None):\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    cols = df.columns\n",
    "    column_values = df.values.ravel()\n",
    "    unique_values =  pd.unique(column_values)\n",
    "    \n",
    "    if not views_ref:\n",
    "        Training = True\n",
    "        alpha = {}\n",
    "        for value, label in zip(unique_values, product_gen(string.ascii_uppercase)):\n",
    "            alpha[value] = label    \n",
    "\n",
    "        for col in cols:\n",
    "            df.replace({col: alpha}, inplace = True)\n",
    "\n",
    "        return df, alpha\n",
    "    \n",
    "    # Rename test views according to views_ref\n",
    "    diff_views = [i for i in list(views_ref) + unique_values.tolist() if i not in views_ref.keys()]\n",
    "    upd_views = list(views_ref) + diff_views\n",
    "    alpha = {}\n",
    "    for value, label in zip(upd_views, product_gen(string.ascii_uppercase)):\n",
    "            alpha[value] = label    \n",
    "\n",
    "    for col in cols:\n",
    "        df.replace({col: alpha}, inplace = True)\n",
    "    return df\n",
    "\n",
    "def product_gen(n):\n",
    "    '''\n",
    "    Code reference:\n",
    "    https://stackoverflow.com/questions/6412473/python-assign-letter-of-the-alphabet-to-each-value-in-a-list'''\n",
    "    \n",
    "    for r in itertools.count(1):\n",
    "        for i in itertools.product(n, repeat=r):\n",
    "            yield \"\".join(i)\n",
    "\n",
    "\n",
    "def dict_categ_sequences(sequence, cluster_ref=None, damping=None):\n",
    "    ''' Return a dictionaire clustering strings listed by levenshtein-distancing. \n",
    "    Code reference:\n",
    "    https://stats.stackexchange.com/questions/123060/clustering-a-long-list-of-strings-words-into-similarity-groups '''\n",
    "    \n",
    "    if not cluster_ref:\n",
    "        Training = True\n",
    "         ## Affinity clustering by string distance  \n",
    "        sequence = np.asarray(sequence)\n",
    "        lev_similarity = -1*np.array([[distance.levenshtein(w1, w2) for w1 in sequence] for w2 in sequence])\n",
    "        affprop = AffinityPropagation(affinity=\"precomputed\", damping=damping) ## To explore\n",
    "        affprop.fit(lev_similarity)\n",
    "       \n",
    "        ## Run dictionaire d        \n",
    "        d = {}  \n",
    "        i = 0\n",
    "        exemplars = []\n",
    "        for cluster_id in np.unique(affprop.labels_):\n",
    "            exemplar = sequence[affprop.cluster_centers_indices_[cluster_id]]\n",
    "            cluster = np.unique(sequence[np.nonzero(affprop.labels_==cluster_id)])\n",
    "            exemplars.append(exemplar)        \n",
    "            key = 'seq_' + str(i)\n",
    "            d[key] = cluster\n",
    "            i += 1\n",
    "                \n",
    "        ## Assembling output from d\n",
    "        newdict = {}\n",
    "        newdict = {i: k for k, v in d.items() for i in v}   \n",
    "        \n",
    "        return newdict, exemplars\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        # for each sequence, calculate the closest reference\n",
    "        reference_list = [ cluster_ref[np.argmin([distance.levenshtein(w1, w2) for w1 in cluster_ref])] for w2 in sequence]\n",
    "        \n",
    "        # create a dictionaire with reference \"keys\" and sequence \"values\"\n",
    "        dct ={k:[x for id,x in enumerate(sequence) if k == reference_list[id]] for k in reference_list}\n",
    "        \n",
    "        # Add missing references to dct and rename keys to seq_():\n",
    "        new_dct ={}\n",
    "        for i, cluster in enumerate(cluster_ref):\n",
    "            if cluster in dct.keys():\n",
    "                new_dct['seq_'+str(i)] = dct[cluster]\n",
    "            else:\n",
    "                new_dct['seq_'+str(i)] = []\n",
    "                \n",
    "        # assembling output form new_dct\n",
    "        newdict = {} \n",
    "        newdict = {i: k for k, v in new_dct.items() for i in v}  \n",
    "        \n",
    "        return newdict\n",
    "\n",
    "\n",
    "def new_categ_features(df, cluster_ref=None, Training=True, damping=0.98):       \n",
    "    '''Generates new features based on the task interpretation'''       \n",
    "    \n",
    "    dataframe = df.copy()\n",
    "    \n",
    "    if cluster_ref:\n",
    "        Training = False\n",
    "    ## Redudancy\n",
    "    dataframe['not_na'] = dataframe.apply(lambda x: x.count()/6, axis=1)    \n",
    "    \n",
    "    ## Diversity\n",
    "    dataframe['diversity'] = [len(set(v[pd.notna(v)]))/6 for v in dataframe.iloc[:,:-1].values]   \n",
    "            \n",
    "    if Training:\n",
    "        ## Sequence\n",
    "        dataframe['sequence'] = dataframe[dataframe.columns[:6]].apply(lambda x: ''.join(x.dropna().astype(str)), axis=1)\n",
    "        sequences = dataframe['sequence'].values\n",
    "        newdict, keys = dict_categ_sequences(sequences, damping=damping)\n",
    "        dataframe['seq_categ'] = dataframe.sequence.str.findall('|'.join(newdict.keys())).str[0].map(newdict)        \n",
    "        \n",
    "        ## Diversity factor\n",
    "        dataframe['divesity_factor'] = round(dataframe.diversity * dataframe.not_na,2)        \n",
    "        ## Drop auxiliars columns\n",
    "        dataframe = dataframe.drop(['diversity', 'not_na', 'sequence'], axis=1)\n",
    "                \n",
    "        return dataframe, keys\n",
    "        \n",
    "    else:\n",
    "        ## Sequence\n",
    "        dataframe['sequence'] = dataframe[dataframe.columns[:6]].apply(lambda x: ''.join(x.dropna().astype(str)), axis=1)\n",
    "        sequences = set(dataframe['sequence'].values)\n",
    "        newdict = dict_categ_sequences(sequences, cluster_ref)\n",
    "        dataframe['seq_categ'] = dataframe.sequence.str.findall('|'.join(newdict.keys())).str[0].map(newdict)   \n",
    "        \n",
    "        ## Diversity factor\n",
    "        dataframe['divesity_factor'] = round(dataframe.diversity * dataframe.not_na,2)\n",
    "        ## Drop auxiliars columns\n",
    "        dataframe = dataframe.drop(['diversity', 'not_na', 'sequence'], axis=1)        \n",
    "    \n",
    "        return dataframe\n",
    "\n",
    "def dummy_new_features(dataframe, valid_cols=None, Training = True):\n",
    "    '''Convert new created categories into dummy variables'''\n",
    "    dm_dataframe = dataframe.copy()\n",
    "    \n",
    "    if valid_cols is not None:\n",
    "        Training = False\n",
    "    \n",
    "    if Training:        \n",
    "        dm_dataframe = pd.get_dummies(dataframe.iloc[:,:-1].astype(str), prefix='', prefix_sep='').max(level=0, axis=1)\n",
    "        dm_dataframe = dm_dataframe.reindex(sorted(dm_dataframe.columns), axis=1)\n",
    "        dm_dataframe['divesity_factor'] = dataframe['divesity_factor']\n",
    "        dm_dataframe = dm_dataframe.iloc[:,:-1].multiply(dm_dataframe['divesity_factor'], axis=\"index\")\n",
    "        dm_dataframe['divesity_factor'] = dataframe.divesity_factor*6\n",
    "        \n",
    "        return dm_dataframe, dm_dataframe.iloc[:,:-1].columns\n",
    "        \n",
    "    else:\n",
    "        dm_dataframe = pd.get_dummies(dataframe.iloc[:,:-1].astype(str), prefix='', prefix_sep='').max(level=0, axis=1)\n",
    "        dm_dataframe = dm_dataframe.reindex(sorted(dm_dataframe.columns), axis=1)        \n",
    "        test_cols = dm_dataframe.columns        \n",
    "        # list columns to remove (which are in  test_cols no in training_views values):\n",
    "        to_drop = []\n",
    "        for col in test_cols:\n",
    "            if 'seq_' not in col:\n",
    "                if col not in valid_cols:\n",
    "                    to_drop.append(col)\n",
    "        dm_dataframe = dm_dataframe.drop(to_drop, axis=1)\n",
    "        # list columns to add (which are not in test_cols but in training_views values):\n",
    "        to_add = []\n",
    "        for col in valid_cols:\n",
    "            if col not in test_cols:\n",
    "                dm_dataframe[col] = 0\n",
    "                \n",
    "        dm_dataframe = dm_dataframe.reindex(sorted(dm_dataframe.columns), axis=1)\n",
    "        dm_dataframe['divesity_factor'] = dataframe['divesity_factor']\n",
    "        dm_dataframe = dm_dataframe.iloc[:,:-1].multiply(dm_dataframe['divesity_factor'], axis=\"index\")\n",
    "        dm_dataframe['divesity_factor'] = dataframe.divesity_factor*6\n",
    "        \n",
    "    return dm_dataframe\n",
    "\n",
    "## FOR SCALARS:\n",
    "def list_constant_columns(dataframe): \n",
    "    '''List columns with no variance'''\n",
    "    to_drop = []\n",
    "    result = dataframe.copy()\n",
    "    for column in dataframe.columns:\n",
    "        \n",
    "        if dataframe[column].std() == 0:\n",
    "            to_drop.append(column)\n",
    "    return to_drop\n",
    "\n",
    "def list_lcorr_columns(dataframe, threshold=0.1):\n",
    "    '''List columns with 'Label-Correlation' lower than threshold'''\n",
    "    to_drop = []\n",
    "    for col in dataframe.columns:\n",
    "        corr = dataframe[col].corr(dataframe['labels'])\n",
    "        if abs(corr) < threshold:\n",
    "            to_drop.append(col)\n",
    "    return to_drop\n",
    "\n",
    "\n",
    "def replace_nan_norm(dataframe, norm_list=None, Training=True):\n",
    "    '''Replace Nan values and normalize according to the training set'''\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    if norm_list:\n",
    "        Training = False\n",
    "        \n",
    "    if Training:\n",
    "        norm_list = []\n",
    "        for col in df.iloc[:,:-1].columns:\n",
    "            mean = np.nanmean(df[col]) \n",
    "            std = np.std(df[col])\n",
    "            df[col] = df[col].fillna(mean)\n",
    "            df[col] =(df[col]-mean )/std            \n",
    "            norm_list.append([mean, std])\n",
    "            \n",
    "        return df, norm_list\n",
    "        \n",
    "    else:\n",
    "        assert dataframe.shape[1] == len(norm_list), 'Different sizes: df={} meanlist={} size board.'.format(dataframe.shape[1], len(mean_list))\n",
    "        for id, col in enumerate(df.columns):\n",
    "            df[col] = df[col].fillna(norm_list[id][0])\n",
    "            df[col] = (df[col]-norm_list[id][0] )/norm_list[id][1]\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "source": [
    "## Importing the datasets..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "5t8AkmZTS5Ga"
   },
   "outputs": [],
   "source": [
    "test_data = pickle.load(open(\"C:/Users/b_tib/coding/Msc/oLINGI2262/Inginious_task_5/test.pickle\", \"rb\"))\n",
    "(train_data_1, train_labels_1) = pickle.load(open(\"C:/Users/b_tib/coding/Msc/oLINGI2262/Inginious_task_5/train1.pickle\", \"rb\"))\n",
    "(train_data_2, train_labels_2) = pickle.load(open(\"C:/Users/b_tib/coding/Msc/oLINGI2262/Inginious_task_5/train2.pickle\", \"rb\"))"
   ]
  },
  {
   "source": [
    "## Splitting categorical and scalar values ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING 1 & 2:\n",
    "train_data = train_data_1.append(train_data_2)\n",
    "train_labels =np.concatenate((train_labels_1, train_labels_2), axis=None)\n",
    "\n",
    "# SEPARTING SCALAR:\n",
    "train_data_scalar = train_data.iloc[:,:1306624]\n",
    "test_data_scalar = test_data.iloc[:,:1306624] # Test (no labels)\n",
    "#train_data_scalar['labels'] = train_labels\n",
    "\n",
    "## Categories\n",
    "train_data_categ_raw = train_data.iloc[:,1306624:]\n",
    "test_data_categ_raw= test_data.iloc[:,1306624:] # Test"
   ]
  },
  {
   "source": [
    "## Processing ....\n",
    "### 1) Divide dataframe into subsets by:\n",
    "#### - Damping categorical according to damping coeficients\n",
    "#### - Renmoving columns with low target-correlation according to threshold\n",
    "### 2) Concatenate damping/thresholds subsets into dct_subsets:\n",
    "#### - Store possible pairwises into a dictionaire\n",
    "#### - For each pairwise, run most importante features (run in google colab)\n",
    "### 3) Run the most relevant features for each dct_subsets and stored in dct_features:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# THIS STEP WAS COMPUTED IN GOOGLE COLAB\n",
    "# THE MOST IMPORTA COLUMNS NAMES ARE STORED IN \"dct_features\".HOWEVER, THERE IS MORE...\n",
    "# THE FUNCTION REPLAN NAN AND NORMILIZE EXTEND TRAIN-SET KNOWNLEDGE TO TEST-SET BEFORE PREDICTION \n",
    "\n",
    "# PROCESSING CATEGORIES\n",
    "dampings = [0.9] #  0.95, 0.98\n",
    "dct_categ = {}\n",
    "for damping in dampings:\n",
    "  try:\n",
    "      ## Convert views into alphabethic variable for distancing\n",
    "      train_data_angles, training_views = alpha_views(train_data_angles) # for colab: remove \"-\" \n",
    "      test_data_angles = alpha_views(test_data_angles, training_views) # for colab: remove \"-\" \n",
    "\n",
    "      ## Applying edit-distancing and clustering using AffinityPropagation\n",
    "      nf_train_data_angles, keys = new_categ_features(train_data_angles, damping=damping) \n",
    "      nf_test_data_angles = new_categ_features(test_data_angles, keys)\n",
    "\n",
    "      ## Converting new categories in dummy variables\n",
    "      train_data_categ, valid_columns = dummy_new_features(nf_train_data_angles) \n",
    "      test_data_categ = dummy_new_features(nf_test_data_angles, valid_cols=valid_columns)\n",
    "\n",
    "      dct_categ[damping] = [train_data_categ, test_data_categ]\n",
    "      \n",
    "  except Exception:\n",
    "      pass  # or you could use 'continue'\n",
    "\n",
    "# PROCESSING SCALARS\n",
    "\n",
    "thresholds = [0.1] #0.2, 0.3\n",
    "dct_scalar = {}\n",
    "for threshold in thresholds:\n",
    "  try:\n",
    "      ## listing columns to drop based on low correlation threshold based on train-set:\n",
    "      drop_constants = list_constant_columns(train_data_vector) # for colab: remove \"-\" \n",
    "      drop_low_corr = list_lcorr_columns(train_data_vector, threshold = threshold) # for colab: remove \"-\"  \n",
    "\n",
    "      ## Dropping columns to drop based on low correlation threshold:\n",
    "      dp_train_data_vector = train_data_vector.drop(drop_constants, axis=1) # for colab: remove \"-\" \n",
    "      dp_train_data_vector = dp_train_data_vector.drop(drop_low_corr, axis=1)\n",
    "\n",
    "      dp_test_data_vector = test_data_vector.drop(drop_constants, axis=1) # for colab: remove \"-\" \n",
    "      dp_test_data_vector = dp_test_data_vector.drop(drop_low_corr, axis=1)\n",
    "\n",
    "      ## Replacing Nan values and normalizing columns based on train-set:\n",
    "      train_data_scalar, norm_list = replace_nan_norm(dp_train_data_vector)\n",
    "      test_data_scalar = replace_nan_norm(dp_test_data_vector, norm_list=norm_list)\n",
    "\n",
    "      dct_scalar[threshold] = [train_data_scalar, test_data_scalar]\n",
    "\n",
    "  except Exception:\n",
    "      pass  # or you could use 'continue'\n",
    "\n",
    "\n",
    "# Concatenating train and test SUBSETS\n",
    "dct_subsets = {}\n",
    "for k_categ, v_categ in dct_categ.items():\n",
    "  for k_scalar, v_scalar in dct_scalar.items():\n",
    "    # merger keys\n",
    "    k_db = str(k_categ) + '_' + str(k_scalar)\n",
    "\n",
    "    # merge values\n",
    "    dct_subsets[k_db] = [pd.concat([v_categ[0], v_scalar[0]], axis=1), pd.concat([v_categ[1], v_scalar[1]], axis=1)]\n",
    "\n",
    "\n",
    "# Passing dataframe through Decision tree importance and getting most significant features for each. \n",
    "dct_features = {}\n",
    "\n",
    "for k, v in dct_subsets.items():\n",
    "\n",
    "  train = v[0]\n",
    "\n",
    "  # DECISION TREE Best Features\n",
    "  x_train = train.iloc[:,:-1]\n",
    "  Y_train = train.iloc[:,-1:]\n",
    "  clf = DecisionTreeClassifier(random_state=0)\n",
    "  clf = clf.fit(x_train, Y_train)\n",
    "  dct= dict(zip(x_train.columns, clf.feature_importances_))\n",
    "  dct = {k: v for k, v in sorted(dct.items(), key=lambda item: item[1], reverse=True)}\n",
    "  features = {x:y for x,y in dct.items() if y!=0}\n",
    "  if features not in dct_features.values():\n",
    "    dct_features[k] = features"
   ]
  },
  {
   "source": [
    "## Feature Selection ...\n",
    "### - Apply damping, thresholds and relevant features for a clean experiemntal databases.\n",
    "### - Using raw (train/test) data, remember to replace Nan by the mean only (standardization during the cross validations).\n",
    "### - *In the raw test data, the Nan replacements is according to train data mean. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "Rr7v5ihR6ujK"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8 0.1\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.8 0.2\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.8 0.3\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 37)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 37)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.95 0.1\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.95 0.2\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.95 0.3\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 34)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 34)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.98 0.1\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.98 0.2\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 21)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 21)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.98 0.3\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 37)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 37)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.9 0.1\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 20)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.9 0.2\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 22)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "0.9 0.3\n",
      "C:\\Users\\b_tib\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n",
      "x_train: (309, 41)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n",
      "x_test: (103, 41)\n",
      "\tThe number of missing Values: 0\n",
      "\tThe average number of missing Values for each attribute: 0.0\n",
      "\tThe standard deviation of the average number of missing Values for each attribute: 0.0\n"
     ]
    }
   ],
   "source": [
    "dct_databases = {}\n",
    "i=0\n",
    "\n",
    "\n",
    "for k, v in dct_features.items():\n",
    "    \n",
    "    damping = float(k.split('_')[0])\n",
    "    threshold = float(k.split('_')[1])\n",
    "    \n",
    "    # PROCESSING CATEGORIES\n",
    "\n",
    "    ## Convert views into alphabethic variable for distancing\n",
    "    train_data_angles, training_views = alpha_views(train_data_categ_raw) \n",
    "    test_data_angles = alpha_views(test_data_categ_raw, training_views) \n",
    "\n",
    "    ## Applying edit-distancing and clustering using AffinityPropagation\n",
    "    nf_train_data_angles, keys = new_categ_features(train_data_angles, damping=damping) # v.split('_')[0]\n",
    "    nf_test_data_angles = new_categ_features(test_data_angles, keys)\n",
    "\n",
    "    ## Converting new categories in dummy variables\n",
    "    train_data_categ, valid_columns = dummy_new_features(nf_train_data_angles) \n",
    "    test_data_categ = dummy_new_features(nf_test_data_angles, valid_cols=valid_columns)\n",
    "    \n",
    "    # MERGING CATEGORIES & SCALARS    \n",
    "    x_train = pd.concat([train_data_categ, train_data], axis=1) # merges into train data because the scalar will be selected autommaticalli by the features\n",
    "    x_test = pd.concat([test_data_categ, test_data], axis=1) # merges into train data because the scalar will be selected autommaticalli by the features\n",
    "    \n",
    "\n",
    "    # FEATURE SELECTION ON RAW DATA \n",
    "    x_train = x_train[v]\n",
    "    x_test = x_test[v]\n",
    "    \n",
    "\n",
    "    # SELECT FEATURES PROCESSING: Remove Nan & \"\"NOT\"\" Normalize    \n",
    "    x_train, mean_list = replace_nan(x_train)\n",
    "    x_test = replace_nan(x_test, mean_list=mean_list)\n",
    "    Y_train = train_labels \n",
    "\n",
    "    \n",
    "    # check all columns are equal between x_train & x_test\n",
    "    assert x_train.columns.all() == x_test.columns.all(), 'train={} and test={} columsn are different.'.format(x_test.shape[1], x_test.shape[1])\n",
    "    \n",
    "    dct_databases[i] = [damping, threshold, x_train, Y_train, x_test]\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "    print('x_train:',x_train.shape)\n",
    "    print('\\tThe number of missing Values:',x_train.isna().sum().sum())\n",
    "    print('\\tThe average number of missing Values for each attribute:',np.mean(x_train.isnull().sum()))\n",
    "    print('\\tThe standard deviation of the average number of missing Values for each attribute:',np.std(x_train.isnull().sum()))\n",
    "\n",
    "    print('x_test:',x_test.shape)\n",
    "    print('\\tThe number of missing Values:',x_test.isna().sum().sum())\n",
    "    print('\\tThe average number of missing Values for each attribute:',np.mean(x_test.isnull().sum()))\n",
    "    print('\\tThe standard deviation of the average number of missing Values for each attribute:',np.std(x_test.isnull().sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Store classes as libraries <----\n",
    "\n",
    "2) Create \"Second\" Bag Class (SVC) and test Single CV \n",
    "\n",
    "3) Create Double (Model comparisson) Nested-CV\n",
    "\n",
    "4) Convert Single/Double Nested-CV into class\n",
    "\n",
    "5) Create a classifier inherates bag classes:  self.super()/decide prediction by votes\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingTrees:\n",
    "    \"\"\"Expand the subset of features in regards each node split for\n",
    "    a more flexible tunning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_t : np.array\n",
    "        Training set features.\n",
    "    Y_t : np.array\n",
    "        Training set labels.\n",
    "    X_v : np.array\n",
    "        Validation set features.\n",
    "    p : list\n",
    "            0: epochs\n",
    "            1: n_trees\n",
    "            2: criterion\n",
    "            3: min_samples_leaf\n",
    "            4: max_depth\n",
    "            5: min_samples_splits\n",
    "            6: max_leaf_nodes\n",
    "    Output\n",
    "    -------\n",
    "    y_pred:      predictions on validation set X_v (array)\n",
    "    unan_rates:  rate of majority votes (array)\n",
    "    acc:         accuracy on training set Y_t (integer)\n",
    "    f1:          f1 score on training set Y_t (integer)\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, p):\n",
    "\n",
    "        # store parameters   \n",
    "        self.epochs = p[0]; self.n_trees = p[1]\n",
    "        self.criterion = p[2]; self.min_samples_leaf = p[3]\n",
    "        self.max_depth = p[4]; self.min_samples_splits = p[5]\n",
    "        self.max_leaf_nodes = p[6]    \n",
    "        \n",
    "        \n",
    "    def fit(self, X_t, Y_t):\n",
    "        \n",
    "        if isinstance(X_t,np.ndarray):\n",
    "            X_t = pd.DataFrame(X_t)\n",
    "        elif not isinstance(X_t,pd.core.frame.DataFrame):\n",
    "            raise Exception('Wrong type for X_t. Expected np.ndarray or pd.DataFrame')\n",
    "\n",
    "        if isinstance(Y_t,np.ndarray):\n",
    "            Y_t = pd.DataFrame(Y_t)\n",
    "        elif not isinstance(X_v,pd.core.frame.DataFrame):\n",
    "            raise Exception('Wrong type for Y_t. Expected np.ndarray or pd.DataFrame')\n",
    "\n",
    "        self.X_t_df = X_t.copy(); self.Y_t_df = Y_t.copy()\n",
    "\n",
    "        X_t['label'] = Y_t\n",
    "        train_df = X_t\n",
    "        for i in range(self.epochs):\n",
    "            self.bag = []\n",
    "            for run in np.arange(self.n_trees):            \n",
    "                # resampling the dataframe (number of distinct, number of distinct)\n",
    "                train_df_bs = train_df.iloc[np.random.randint(len(train_df), size=len(train_df))]\n",
    "                X_train = train_df_bs.iloc[:,:-1]\n",
    "                Y_train = train_df_bs.iloc[:,-1:]\n",
    "                # Storing each trained tree\n",
    "                wl = DecisionTreeClassifier(criterion=self.criterion\n",
    "                                        , min_samples_leaf=self.min_samples_leaf\n",
    "                                        , max_depth=self.max_depth\n",
    "                                        , min_samples_split=self.min_samples_splits\n",
    "                                        , max_leaf_nodes=self.max_leaf_nodes).fit(X_train,Y_train) \n",
    "                                        #, random_state=run  \n",
    "                # add tree into bag\n",
    "                self.bag.append(wl)\n",
    "\n",
    "        ## Score on Training set\n",
    "        t_predictions = []\n",
    "        for i in range(self.n_trees):        \n",
    "            tree_t_prediction = self.bag[i].predict(self.X_t_df) # predict validation and training sets   \n",
    "            t_predictions.append(tree_t_prediction) # Append predictions\n",
    "        \n",
    "        # Convert predictions lists into np.array to transpose them and obtain \"n_tree\" predictions per line\n",
    "        t_predictions_T = np.array(t_predictions).T\n",
    "\n",
    "        t_final_predictions = []\n",
    "        # for each entry \"m\" of X_t_df(m x features)\n",
    "        for line in t_predictions_T:\n",
    "            # countabilize the \"n_tree\" votes in v_predictions_T (m x n_tree)\n",
    "            most_common = Counter(line).most_common(1)[0][0]\n",
    "            t_final_predictions.append(most_common) \n",
    "\n",
    "        # accuracies values\n",
    "        self.acc = accuracy_score(self.Y_t_df, t_final_predictions)\n",
    "        self.f1 = f1_score(self.Y_t_df, t_final_predictions, average='macro')\n",
    "        self.bcr = balanced_accuracy_score(self.Y_t_df, t_final_predictions)\n",
    "        self.auc = roc_auc_score(self.Y_t_df, t_final_predictions, average='macro')\n",
    "        return \n",
    "            \n",
    "    def predict(self, X_v): \n",
    "\n",
    "        if isinstance(X_v,np.ndarray):\n",
    "            X_v = pd.DataFrame(X_v)\n",
    "        elif not isinstance(X_v,pd.core.frame.DataFrame):\n",
    "            raise Exception('Wrong type for X_v. Expected np.ndarray or pd.DataFrame')\n",
    "\n",
    "        self.X_v_df = X_v.copy()\n",
    "        ## Prediction on Validation set\n",
    "        v_predictions = []\n",
    "        # each tree will make a prediction about test_df\n",
    "        for i in range(self.n_trees):\n",
    "            tree_v_prediction = self.bag[i].predict(self.X_v_df) # predict validation and training sets\n",
    "            v_predictions.append(tree_v_prediction) # Append predictions\n",
    "        # Convert predictions lists into np.array to transpose them and obtain \"n_tree\" predictions per line\n",
    "        v_predictions_T = np.array(v_predictions).T\n",
    "        \n",
    "        self.prediction = []   \n",
    "        self.votes = [] \n",
    "        # for each entry \"n\" of X_v_df(n x features)\n",
    "        for line in v_predictions_T:\n",
    "            # countabilize the \"n_tree\" votes in v_predictions_T (n x n_tree) \n",
    "            most_common = Counter(line).most_common(1)[0][0]\n",
    "            unanimity_rate = Counter(line)[most_common] / len(line)\n",
    "            # get prediction and unanimity rate\n",
    "            self.prediction.append(most_common)\n",
    "            self.votes.append(unanimity_rate)\n",
    "        return self.prediction\n",
    "        \n",
    "\n"
   ]
  },
  {
   "source": [
    "## Validations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# * FINISH SINGLE-MODEL NESTED AND THEM ADD THE DOUBLE-MODEL NESTE\n",
    "# ** AFTER MODIFIED THE BAGGIN INTO CLASS, MAKE CV-NESTED GENERICAL \n",
    "##########################################################################\n",
    "\n",
    "def nested_single_cv(x_t, y_t, L, grid, k_ext, k_int):\n",
    "    \"\"\"   \n",
    "    Help set a hyper-parameters list for a given model before makes \n",
    "    its comparison with others hyper-parameterized models.\n",
    "        \n",
    "    Input: \n",
    "        - x_t: features train (numpy.arrays)\n",
    "        - y_t: labels train (numpy.arrays)\n",
    "        - L: learning algorithm (class method .predict())\n",
    "        - grid: keys as a parameter name; values as the array of the parameter' values (dict)        \n",
    "        - K_ext: number of external folds (integer) \n",
    "        - K_int: number of internal folds (integer)\n",
    "\n",
    "    Output:        \n",
    "        - inner_result_frame: index: [k_ext], columns: [hp_set], values: [v_bcr_(k_int_mean)]\n",
    "        - outter_result_frame: index: [k_ext, hp_hat], columns:[t_bcr, v_bcr], values:[t_bcr, v_bcr]\n",
    "        - outter_result_aggre: index: [hp_hat], columns:[t_bcr_(ext_mean), v_bcr_(ext_mean), v_bcr_(ext_std)]\n",
    "    \"\"\"\n",
    "    \n",
    "    hp_set = [v for v in product(*grid.values())]\n",
    "\n",
    "    inner_results = pd.DataFrame(columns = hp_set)\n",
    "\n",
    "    outter_results = pd.DataFrame(columns = ['hp_hat' \n",
    "                                        , 't_bcr'\n",
    "                                        , 'v_bcr'\n",
    "                                        ])\n",
    "\n",
    "    \n",
    "    # frame pointer\n",
    "    i = 0\n",
    "    # partionate \"training rows\" into \"K_ext\" sets   \n",
    "    K_ext_folds = KFold(n_splits = k_ext, shuffle=False).split(x_t)  # (markers t_i, v_i)\n",
    "    for t_ext_fold, v_ext_fold in K_ext_folds:\n",
    "        # sectioning \"train set\" between \"S_k\" into \"ext_fold\" sets            \n",
    "        x_S_k = x_t[t_ext_fold] # training x\n",
    "        y_S_k = y_t[t_ext_fold] # training y            \n",
    "        x_ext_fold = x_t[v_ext_fold] # test x \n",
    "        y_ext_fold = y_t[v_ext_fold] # test y\n",
    "        \n",
    "        # get hp_hat in the inner loop\n",
    "        hp_dic = {}\n",
    "        for idx, hp in enumerate(hp_set):  \n",
    "            hp_dic[idx]=[]\n",
    "            # partionate \"S_k training rows\" into \"K_int\" sets\n",
    "            K_int_folds = KFold(n_splits = k_int, shuffle=False).split(x_S_k)   \n",
    "            for t_int_fold, v_int_fold in K_int_folds:\n",
    "                # sectioning \"S_k\" between \"Ss_k\" into \"int_fold\" sets                \n",
    "                x_Ss_k = x_S_k[t_int_fold] # training x\n",
    "                y_Ss_k = y_S_k[t_int_fold] # training y  \n",
    "                x_int_fold = x_S_k[v_int_fold] # test x\n",
    "                y_int_fold = y_S_k[v_int_fold] # test y\n",
    "                \n",
    "                # must scaler after partition, for specific a training normalization\n",
    "                min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                X_t = min_max_scaler.fit_transform(x_Ss_k)\n",
    "                X_v = min_max_scaler.fit_transform(x_int_fold)\n",
    "                Y_t = y_Ss_k\n",
    "                Y_v = y_int_fold\n",
    "                \n",
    "                # Loading and fitting model\n",
    "                model = L(hp)\n",
    "                model.fit(X_t, Y_t)\n",
    "                # prediction\n",
    "                Y_v_predicted = model.predict(X_v)             \n",
    "                # validation\n",
    "                v_bcr = balanced_accuracy_score(Y_v, Y_v_predicted)\n",
    "                # append all \n",
    "                hp_dic[idx].append(v_bcr)\n",
    "        # # Averages the k_int iteractions for each hp in hp_set and stores it\n",
    "        inner_results.loc[i] = [sum(arr) / len(arr) for arr in hp_dic.values()]\n",
    "\n",
    "        \n",
    "        # avg all hp predictions scores and define the higher to hp_hat\n",
    "        ixd_max= max([(k,np.mean(v)) for k,v in hp_dic.items()],key=lambda item:item[1])[0]\n",
    "        hp_hat = hp_set[ixd_max]  \n",
    "\n",
    "        \n",
    "        # must scaler after partition, for specific a training normalization\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_t = min_max_scaler.fit_transform(x_S_k)\n",
    "        X_v = min_max_scaler.fit_transform(x_ext_fold)\n",
    "        Y_t = y_S_k\n",
    "        Y_v = y_ext_fold\n",
    "\n",
    "        # Loading and fitting model\n",
    "        model = L(hp)\n",
    "        model.fit(X_t, Y_t)\n",
    "        # prediction\n",
    "        Y_v_predicted = model.predict(X_v)   \n",
    "\n",
    "        # training metrics \n",
    "        t_acc = model.acc\n",
    "        t_bcr = model.bcr\n",
    "        t_f1 = model.f1\n",
    "        t_auc = model.auc \n",
    "        # validation metrics\n",
    "        v_acc = accuracy_score(Y_v, Y_v_predicted)\n",
    "        v_bcr = balanced_accuracy_score(Y_v, Y_v_predicted)\n",
    "        v_f1 = f1_score(Y_v, Y_v_predicted, average='macro')\n",
    "        v_auc = roc_auc_score(Y_v, Y_v_predicted, average='macro')\n",
    "        \n",
    "        outter_results.loc[i] = [hp_hat \n",
    "                                , t_bcr\n",
    "                                , v_bcr]                               \n",
    "        i += 1\n",
    "\n",
    "\n",
    "    return outter_results, inner_results\n"
   ]
  },
  {
   "source": [
    "## Importing featured selected database"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# OPERATE SINGEL AND DOUBLE NESTED LOOPS FOR MULTIPLE MODEL AND PARAMETERS\n",
    "# * Plot distribution graphs\n",
    "##########################################################################\n",
    "\n",
    "# import...\n",
    "x_train = pd.read_csv('x_train.csv')\n",
    "y_train = pd.read_csv('Y_train.csv')\n",
    "x_test = pd.read_csv('x_test.csv')\n",
    "\n",
    "# convert o numpy\n",
    "x_t = x_train.values\n",
    "y_t = y_train.values\n",
    "x_v = x_test.values\n"
   ]
  },
  {
   "source": [
    "# ... Single Model Validations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                  hp_hat t_bcr                         v_bcr  \\\n",
       "                                         count      mean       std      mean   \n",
       "0  (1, 100, entropy, 0.06, 3, 0.03, 200)     4  0.819879  0.042065  0.711106   \n",
       "\n",
       "             \n",
       "        std  \n",
       "0  0.028903  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>hp_hat</th>\n      <th colspan=\"3\" halign=\"left\">t_bcr</th>\n      <th colspan=\"2\" halign=\"left\">v_bcr</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(1, 100, entropy, 0.06, 3, 0.03, 200)</td>\n      <td>4</td>\n      <td>0.819879</td>\n      <td>0.042065</td>\n      <td>0.711106</td>\n      <td>0.028903</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "source": [
    "\n",
    "# FIND THE HYPER PARAMETERS FOR EACH MODEL CASTED\n",
    "\n",
    "# Model1: Bagging_trees\n",
    "model1= BaggingTrees\n",
    "grid1 = {'epochs':[1]\n",
    "                    , 'n_trees':[100]\n",
    "                    , 'criterion': ['entropy']\n",
    "                    , 'min_samples_leaf':[0.06] #\n",
    "                    , 'max_depth':[3]\n",
    "                    , 'min_samples_split':[0.03] # \n",
    "                    , 'max_leaf_nodes':[200]\n",
    "                    }\n",
    "\n",
    "\n",
    "# Model2: SVC\n",
    "grid2 = {'C': np.arange( 1, 50+1, 10).tolist()\n",
    "                    , 'kernel': ['rbf', 'sigmoid', 'poly']\n",
    "                    , 'gamma':  np.arange( 0, 3, 1).tolist()\n",
    "                    , 'coef0': np.arange( 0, 10, 2 ).tolist()        \n",
    "                    , 'degree': np.arange( 0, 100+0, 1 ).tolist()\n",
    "                    , 'tol': np.arange( 0.001, 0.01, 0.003 ).tolist()\n",
    "                    #, 'shrinking': [True]\n",
    "                    #, 'probability': [False] \n",
    "                    #, 'cache_size':   [2000]\n",
    "                    #, 'class_weight': [None]\n",
    "                    #, 'verbose':      [False]\n",
    "                    #, 'max_iter':     [-1]\n",
    "                    #, 'random_state': [None]\n",
    "                    }\n",
    "\n",
    "\n",
    "# model2= SVC()\n",
    "\n",
    "\n",
    "K_int, K_ext = 4, 10 # internal should simulate data prop. while external go for true prediction p\n",
    "\n",
    "# only training set\n",
    "outter, inner = nested_single_cv(x_t, y_t, model1, grid1, K_ext, K_int)\n",
    "\n",
    "outter.groupby('hp_hat').agg({'t_bcr': ['count', 'mean', 'std']\n",
    "                              , 'v_bcr': ['mean', 'std']\n",
    "                              }).reset_index('hp_hat')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                hp_hat     t_bcr     v_bcr\n",
       "0  (1, 7, entropy, 0.02, 3, 0.05, 200)  0.820525  0.762069\n",
       "1  (1, 7, entropy, 0.02, 3, 0.05, 200)  0.827016  0.788462\n",
       "2  (1, 7, entropy, 0.02, 3, 0.03, 200)  0.846765  0.660326\n",
       "3  (1, 7, entropy, 0.02, 3, 0.05, 200)  0.851765  0.788043\n",
       "4  (1, 7, entropy, 0.02, 3, 0.03, 200)  0.785334  0.787037\n",
       "5  (1, 7, entropy, 0.02, 3, 0.03, 200)  0.865002  0.610769\n",
       "6  (1, 7, entropy, 0.02, 3, 0.03, 200)  0.832893  0.698880\n",
       "7  (1, 7, entropy, 0.02, 3, 0.05, 200)  0.825246  0.736842"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hp_hat</th>\n      <th>t_bcr</th>\n      <th>v_bcr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(1, 7, entropy, 0.02, 3, 0.05, 200)</td>\n      <td>0.820525</td>\n      <td>0.762069</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(1, 7, entropy, 0.02, 3, 0.05, 200)</td>\n      <td>0.827016</td>\n      <td>0.788462</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(1, 7, entropy, 0.02, 3, 0.03, 200)</td>\n      <td>0.846765</td>\n      <td>0.660326</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(1, 7, entropy, 0.02, 3, 0.05, 200)</td>\n      <td>0.851765</td>\n      <td>0.788043</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(1, 7, entropy, 0.02, 3, 0.03, 200)</td>\n      <td>0.785334</td>\n      <td>0.787037</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(1, 7, entropy, 0.02, 3, 0.03, 200)</td>\n      <td>0.865002</td>\n      <td>0.610769</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(1, 7, entropy, 0.02, 3, 0.03, 200)</td>\n      <td>0.832893</td>\n      <td>0.698880</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(1, 7, entropy, 0.02, 3, 0.05, 200)</td>\n      <td>0.825246</td>\n      <td>0.736842</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "# columns are HPs, and rows internal loops\n",
    "outter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   (1, 7, entropy, 0.02, 3, 0.03, 200)  (1, 7, entropy, 0.02, 3, 0.05, 200)  \\\n",
       "0                             0.722145                             0.757435   \n",
       "1                             0.682905                             0.724578   \n",
       "2                             0.735534                             0.693210   \n",
       "3                             0.669511                             0.713237   \n",
       "4                             0.737739                             0.718031   \n",
       "5                             0.735603                             0.719499   \n",
       "6                             0.718601                             0.689860   \n",
       "7                             0.659224                             0.689411   \n",
       "\n",
       "   (1, 7, entropy, 0.03, 3, 0.03, 200)  (1, 7, entropy, 0.03, 3, 0.05, 200)  \n",
       "0                             0.708206                             0.728526  \n",
       "1                             0.717874                             0.701348  \n",
       "2                             0.699702                             0.699609  \n",
       "3                             0.686127                             0.636010  \n",
       "4                             0.698645                             0.706383  \n",
       "5                             0.710399                             0.725520  \n",
       "6                             0.692890                             0.701952  \n",
       "7                             0.642939                             0.673328  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(1, 7, entropy, 0.02, 3, 0.03, 200)</th>\n      <th>(1, 7, entropy, 0.02, 3, 0.05, 200)</th>\n      <th>(1, 7, entropy, 0.03, 3, 0.03, 200)</th>\n      <th>(1, 7, entropy, 0.03, 3, 0.05, 200)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.722145</td>\n      <td>0.757435</td>\n      <td>0.708206</td>\n      <td>0.728526</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.682905</td>\n      <td>0.724578</td>\n      <td>0.717874</td>\n      <td>0.701348</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.735534</td>\n      <td>0.693210</td>\n      <td>0.699702</td>\n      <td>0.699609</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.669511</td>\n      <td>0.713237</td>\n      <td>0.686127</td>\n      <td>0.636010</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.737739</td>\n      <td>0.718031</td>\n      <td>0.698645</td>\n      <td>0.706383</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.735603</td>\n      <td>0.719499</td>\n      <td>0.710399</td>\n      <td>0.725520</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.718601</td>\n      <td>0.689860</td>\n      <td>0.692890</td>\n      <td>0.701952</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.659224</td>\n      <td>0.689411</td>\n      <td>0.642939</td>\n      <td>0.673328</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "inner"
   ]
  },
  {
   "source": [
    "## Figuring out my submission"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 100, 'entropy', 0.06, 3, 0.02, 200]\n",
      "[1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "votes [0.89, 0.87, 0.95, 0.98, 0.92, 0.72, 0.96, 0.98, 0.9, 0.81, 0.91, 0.75, 0.79, 0.77, 0.75, 0.86, 0.91, 0.98, 0.76, 0.78, 1.0, 0.87, 0.71, 0.91, 0.51, 0.86, 0.9, 1.0, 0.97, 0.98, 0.91, 0.82, 1.0, 0.97, 0.76, 0.76, 0.86, 0.65, 0.77, 0.53, 0.92, 0.95, 0.79, 0.99, 0.97, 0.96, 0.57, 0.8, 0.97, 0.93, 0.56, 1.0, 0.72, 0.77, 0.62, 0.81, 0.89, 0.64, 0.9, 0.66, 0.94, 0.8, 0.74, 0.92, 0.91, 0.77, 1.0, 0.71, 0.98, 0.85, 0.98, 0.95, 0.82, 0.58, 0.95, 0.72, 0.95, 0.8, 0.74, 0.99, 0.75, 0.53, 0.99, 0.99, 0.9, 0.54, 0.97, 0.63, 0.81, 0.75, 0.72, 0.67, 0.99, 0.68, 0.96, 1.0, 0.85, 0.66, 0.9, 0.91, 0.94, 0.97, 0.95]\n",
      "[1, 100, 'entropy', 0.03, 3, 0.06, 200]\n",
      "[1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "votes [0.77, 0.83, 0.93, 0.97, 0.93, 0.79, 0.98, 0.99, 0.92, 0.83, 0.88, 0.61, 0.6, 0.67, 0.78, 0.83, 0.85, 0.98, 0.85, 0.89, 0.99, 0.93, 0.78, 0.9, 0.58, 0.84, 0.78, 1.0, 0.97, 0.96, 0.94, 0.79, 0.99, 0.98, 0.76, 0.84, 0.79, 0.67, 0.87, 0.58, 0.94, 0.95, 0.87, 1.0, 0.99, 0.98, 0.53, 0.84, 1.0, 0.92, 0.51, 1.0, 0.74, 0.84, 0.67, 0.67, 0.76, 0.5, 0.95, 0.57, 0.95, 0.57, 0.69, 0.93, 0.79, 0.7, 1.0, 0.6, 0.99, 0.93, 0.98, 0.99, 0.77, 0.59, 0.97, 0.74, 0.96, 0.75, 0.84, 0.95, 0.78, 0.55, 1.0, 0.98, 0.91, 0.59, 0.97, 0.53, 0.92, 0.87, 0.84, 0.59, 0.98, 0.81, 0.98, 1.0, 0.79, 0.62, 0.93, 0.85, 0.92, 0.99, 0.97]\n",
      "[1, 100, 'entropy', 0.02, 3, 0.03, 200]\n",
      "[1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "votes [0.78, 0.77, 0.99, 0.98, 0.98, 0.83, 0.97, 0.97, 0.91, 0.87, 0.87, 0.77, 0.72, 0.75, 0.87, 0.8, 0.82, 0.99, 0.87, 0.9, 1.0, 0.91, 0.82, 0.91, 0.62, 0.88, 0.78, 1.0, 0.96, 0.98, 0.94, 0.73, 1.0, 0.98, 0.78, 0.91, 0.69, 0.7, 0.84, 0.65, 0.94, 0.96, 0.88, 0.99, 0.98, 0.99, 0.6, 0.91, 0.99, 0.97, 0.5, 1.0, 0.73, 0.9, 0.69, 0.73, 0.77, 0.57, 0.91, 0.5, 0.91, 0.73, 0.57, 0.99, 0.75, 0.68, 1.0, 0.52, 1.0, 0.91, 0.99, 0.97, 0.77, 0.59, 0.96, 0.68, 1.0, 0.75, 0.87, 0.98, 0.74, 0.62, 1.0, 1.0, 0.86, 0.62, 1.0, 0.6, 0.86, 0.82, 0.82, 0.66, 0.99, 0.85, 0.98, 1.0, 0.84, 0.59, 0.98, 0.92, 0.91, 1.0, 0.99]\n"
     ]
    }
   ],
   "source": [
    "# convert o numpy\n",
    "x_t = x_train.values\n",
    "y_t = y_train.values\n",
    "x_v = x_test.values\n",
    "\n",
    "\n",
    "# normalize (np.ndarray)\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_t = min_max_scaler.fit_transform(x_t)\n",
    "x_v = min_max_scaler.fit_transform(x_v)\n",
    "\n",
    "                                    #cnt\tt_bcr_mean\tt_bcr_std\tv_bcr_mean\tv_bcr_std\n",
    "p0 = [1, 100, 'entropy', 0.06, 3, 0.02, 200]    # 4\t    0.822906\t0.045762\t0.728306\t0.034401\n",
    "p2 = [1, 100, 'entropy', 0.03, 3, 0.06, 200]\t    # 4\t    0.846455\t0.052602\t0.726517\t0.04923\n",
    "p3 = [1, 100, 'entropy', 0.02, 3, 0.03, 200]\n",
    "\n",
    "for p in [p0, p2, p3]:\n",
    "    model = BaggingTrees(p)\n",
    "    model.fit(x_t, y_t)\n",
    "    print(p)\n",
    "    print(model.predict(x_v))\n",
    "    print('votes',model.votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damping</th>\n",
       "      <th>thres</th>\n",
       "      <th>TrainAcc_mean</th>\n",
       "      <th>TrainAcc_std</th>\n",
       "      <th>TestAcc_mean</th>\n",
       "      <th>TestAcc_std</th>\n",
       "      <th>BCR_mean</th>\n",
       "      <th>BCR_std</th>\n",
       "      <th>criterion</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.789657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   damping  thres  TrainAcc_mean  TrainAcc_std  TestAcc_mean  TestAcc_std  \\\n",
       "0      0.9    0.1       0.789657           0.0       0.75743          0.0   \n",
       "\n",
       "   BCR_mean  BCR_std criterion  min_samples_leaf max_depth  min_samples_split  \\\n",
       "0  0.740518      0.0   entropy              0.01         2               0.05   \n",
       "\n",
       "  max_leaf_nodes  \n",
       "0            200  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree # k-fold5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damping</th>\n",
       "      <th>thres</th>\n",
       "      <th>TrainAcc_mean</th>\n",
       "      <th>TrainAcc_std</th>\n",
       "      <th>TestAcc_mean</th>\n",
       "      <th>TestAcc_std</th>\n",
       "      <th>BCR_mean</th>\n",
       "      <th>BCR_std</th>\n",
       "      <th>criterion</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.821193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.731253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   damping  thres  TrainAcc_mean  TrainAcc_std  TestAcc_mean  TestAcc_std  \\\n",
       "0      0.9    0.1       0.821193           0.0      0.731253          0.0   \n",
       "\n",
       "   BCR_mean  BCR_std criterion  min_samples_leaf max_depth  min_samples_split  \\\n",
       "0  0.717038      0.0   entropy              0.01         2               0.05   \n",
       "\n",
       "  max_leaf_nodes  \n",
       "0            200  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag # k-fold5, 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damping</th>\n",
       "      <th>thres</th>\n",
       "      <th>TrainAcc_mean</th>\n",
       "      <th>TrainAcc_std</th>\n",
       "      <th>TestAcc_mean</th>\n",
       "      <th>TestAcc_std</th>\n",
       "      <th>BCR_mean</th>\n",
       "      <th>BCR_std</th>\n",
       "      <th>criterion</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.797195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.702214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.677592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   damping  thres  TrainAcc_mean  TrainAcc_std  TestAcc_mean  TestAcc_std  \\\n",
       "0      0.9    0.1       0.797195           0.0      0.702214          0.0   \n",
       "\n",
       "   BCR_mean  BCR_std criterion  min_samples_leaf max_depth  min_samples_split  \\\n",
       "0  0.677592      0.0   entropy              0.01         2               0.05   \n",
       "\n",
       "  max_leaf_nodes  \n",
       "0            200  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree # k-fold4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damping</th>\n",
       "      <th>thres</th>\n",
       "      <th>n_tree</th>\n",
       "      <th>TrainAcc_mean</th>\n",
       "      <th>TrainAcc_std</th>\n",
       "      <th>TestAcc_mean</th>\n",
       "      <th>TestAcc_std</th>\n",
       "      <th>BCR_mean</th>\n",
       "      <th>BCR_std</th>\n",
       "      <th>criterion</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.862446</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.772467</td>\n",
       "      <td>0.060262</td>\n",
       "      <td>0.783448</td>\n",
       "      <td>0.069478</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   damping  thres n_tree  TrainAcc_mean  TrainAcc_std  TestAcc_mean  \\\n",
       "0      0.9    0.1    100       0.862446      0.015038      0.772467   \n",
       "\n",
       "   TestAcc_std  BCR_mean   BCR_std criterion  min_samples_leaf max_depth  \\\n",
       "0     0.060262  0.783448  0.069478   entropy              0.06         3   \n",
       "\n",
       "   min_samples_split max_leaf_nodes  \n",
       "0               0.02            200  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag # k-fold4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x288483306a0>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABH4AAAGbCAYAAACyBFePAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABP00lEQVR4nO3deZzVdd3//8f7zMLAMOyLyKq4gIiioqJCYWquuWRlptZVWpZpZllXy/W7vld9v1eb2mrmlpdmWprVpZZLZu4giogCLrghm8gOwwwzzMx5//74zJEBWQaYOevj7u3czpnz+czn8xr0qDx5vV/vEGNEkiRJkiRJxSeV6wIkSZIkSZLUOQx+JEmSJEmSipTBjyRJkiRJUpEy+JEkSZIkSSpSBj+SJEmSJElFqjybN+vXr18cMWJENm8pSZIkSZJU1J577rnlMcb+WzqW1eBnxIgRTJ8+PZu3lCRJkiRJKmohhLe3dsylXpIkSZIkSUXK4EeSJEmSJKlIGfxIkiRJkiQVqazO+JEkSZIkSYWtqamJhQsX0tDQkOtSSk5VVRVDhgyhoqKi3d9j8CNJkiRJktpt4cKF1NTUMGLECEIIuS6nZMQYWbFiBQsXLmSPPfZo9/e51EuSJEmSJLVbQ0MDffv2NfTJshACffv23eFOK4MfSZIkSZK0Qwx9cmNnft0NfiRJkiRJkoqUwY8kSZIkSVKRMviRJEmSJElF59FHH+WUU04BoLGxkWOPPZZx48Zxxx13tOv7X3nlFcaNG8dBBx3EG2+88d77hx9+OOPGjWPYsGH079+fcePGMW7cOObNm7dL9X73u99l6NChdO/efZeuszl39ZIkSZIkSUXt+eefB2DmzJnt/p7//d//5WMf+xj/8R//scn706ZNA+Dmm29m+vTpXH311R1S40c+8hEuvvhi9t577w65XobBjyRJkiRJ2jkXXtg5173uuq0emjdvHieccAKHHHIIM2bMYMyYMfzud7+jW7duPPDAA3z1q1+lW7duTJw4EYClS5dy7rnnsmzZMsaNG8ef//xnRo4c+d71Zs6cyRe/+EXq6+sZOXIkN910E1OnTuXnP/85ZWVlPPzwwzzyyCM7/aN0796dSy+9lL/97W907dqVu+++m4EDB77vvAkTJuz0PbbFpV6SJEmSJKmgvPrqq1x00UW8/PLL9OjRg2uuuYaGhgY+//nPc++99/Lcc8+xZMkSAAYMGMCNN97IpEmTmDlz5iahD8CnP/1pfvzjH/Piiy8yduxYvve973HSSSfxxS9+kcsuu2yXQh+Auro6JkyYwAsvvMAHPvABbrjhhl263o6y40eSJEmSJO2cbXTmdKahQ4dy1FFHAXDuuefyy1/+kmOPPZY99tjjvaVS5557Ltdff/02r7NmzRpWr17NBz/4QQA+85nP8PGPf7xDa62srHxv1tAhhxzCQw891KHX3x47fiRJkiRJhenFF2HVqlxXoRwIIWzz63xSUVHxXn1lZWU0NzfT0tLy3lDo//zP/+zU+xv8SJIkSZIKz9tvw69/DbfdlutKlAPz589n6tSpANx+++1MnDiRUaNGMW/evPd24PrDH/6w3ev07NmT3r1788QTTwBw6623vtf905nKysqYOXMmM2fO5Pvf/36n3svgR5IkSZJUeJYuTZ5XrMhtHcqJfffdl1//+teMHj2aVatW8aUvfYmqqiquv/56Tj75ZA4++GAGDBjQrmvdcsstfOMb3+CAAw5g5syZnd6BszXf/OY3GTJkCPX19QwZMoT/+q//6pDrhhhjh1yoPcaPHx+nT5+etftJkiRJkorUQw/BXXdBjx5wxRW5rqakvPzyy4wePTpn9583bx6nnHIKs2fPzlkNubSlX/8QwnMxxvFbOt+OH0mSJElS4Vm9Onletw6y2NAgFRp39ZIkSZIkFZ5M8JNOw/r10K1bTstR9owYMSIvu30OP/xwGhsbN3nv1ltvZezYsTmqKGHwI0mSJEkqPGvWbHy9bp3Bj3Ju2rRpuS5hi7a71CuEUBVCeCaE8EIIYU4I4Xut798cQngrhDCz9TGu06uVJEmSJAk2dvxAEvxI2qL2dPw0Ah+KMa4LIVQAT4YQ7m899o0Y412dV54kSZIkSZuJ0eBHaqftBj8x2fYr8ymqaH04OUuSJEmSlBvr10NT08avDX6krWrXrl4hhLIQwkxgKfBQjDGzcO2/QwgvhhB+FkLospXv/UIIYXoIYfqyZcs6pmpJkiRJUulq2+0DUFubkzKkQtCu4CfG2BJjHAcMAQ4LIewPfBsYBRwK9AH+fSvfe32McXyMcXz//v07pmpJkiRJUulqO9gZ7PgpQatXr+aaa67Z4e979NFHOeWUUzqhovzVruAnI8a4GngEOCHG+E5MNAL/AxzWCfVJkiRJkrSpTMdPCMmzwU/J2dngpxRtd8ZPCKE/0BRjXB1C6AocB/w4hDAoxvhOCCEApwOzO7dUSZIkSZLYGPz07w9Llxr85NCFF3bOda+7btvHv/Wtb/HGG28wbtw4jjvuOK644opNjscY+eY3v8n9999PCIH/+I//4KyzzgJg7dq1nHzyybz++uscffTRXHPNNcQYOf/885k+fTohBD73uc9x2WWXdc4Pl2Xt2dVrEHBLCKGMpEPozhjj30II/2oNhQIwE/hi55UpSZIkSVKrzFKvIUMMfkrUj370I2bPns3MmTO3ePwvf/kLM2fO5IUXXmD58uUceuihfOADHwDgmWee4aWXXmL48OGccMIJ/OUvf2GPPfZg0aJFzJ6d9LSs3nyOVAFrz65eLwIHbeH9D3VKRZIkSZIkbUvmN+VDh8KMGQY/ObS9zpxcefLJJzn77LMpKytj4MCBfPCDH+TZZ5+lR48eHHbYYey5554AnH322Tz55JMcc8wxvPnmm1xyySWcfPLJfPjDH87xT9BxdmjGjyRJkiRJOZcJfgYPTp4NfrQDQmY2VJuve/fuzQsvvMDkyZO59tprueCCC3JUXccz+JEkSZIkFZZM8DNoUDLgub4eWlpyWpKyq6amhtra2q0enzRpEnfccQctLS0sW7aMxx9/nMMOS/akeuaZZ3jrrbdIp9PccccdTJw4keXLl5NOpznzzDP5f//v/zFjxoxs/SidzuBHkiRJklQ4Ytw446d3b6iuTl7X1eWuJmVd3759Oeqoo9h///35xje+8b7jZ5xxBgcccAAHHnggH/rQh/jJT37CbrvtBsChhx7KxRdfzOjRo9ljjz0444wzWLRoEZMnT2bcuHGce+65/PCHP8z2j9Rp2jPcWZIkSZKk/LBuHaTTSeBTUQE1Ncl769ZBjx65rk5ZdPvtt2/1WAiBK6644n27fU2ePJnHH3/8fecfeOCBRdXl05YdP5IkSZKkwpFZ5tWzZ/LcvXvy7JwfaYvs+JEkSZIkFY62y7zA4KfEzZo1i/POO2+T97p06cK0adNyVFH+MfiRJEmSJBUOO37UxtixY5k5c2auy8hrLvWSJEmSJBWOTPDTq1fybPAjbZPBjyRJkiSpcGyt42cbW3tLpczgR5IkSZJUODIzfuz4kdrF4EeSJEmSVDhc6iVg9erVXHPNNbkuoyAY/EiSJEmSCofBjzD42REGP5IkSZKkwpBOJ7N8QoAePZL3amqSZ4OfkvKtb32LN954g3HjxvGNb3zjfcfXrVvHMcccw8EHH8zYsWO5++67c1BlfnA7d0mSJElSYVizBmJMQp9Uax+Dw51z6sJ7L+yU6173keu2efxHP/oRs2fP3upW7lVVVfz1r3+lR48eLF++nAkTJnDqqacSQuiEavObwY8kSZIkqTBsPtgZoLISysuhqQk2bEi+VsmLMfKd73yHxx9/nFQqxaJFi3j33XfZbbfdcl1a1hn8SJIkSZIKw+bzfSBZ9tW9e3Js3Tro0ycHhZWu7XXm5Mptt93GsmXLeO6556ioqGDEiBE0NDTkuqyccMaPJEmSJKkwZIKfnj03fd8BzyWnpqaG2m0s71uzZg0DBgygoqKCRx55hLfffjuL1eUXgx9JkiRJUmHY0lIvcMBzCerbty9HHXUU+++//xaHO59zzjlMnz6dsWPH8rvf/Y5Ro0bloMr84FIvSZIkSVJh2NJSL3DAc4m6/fbbt3qsX79+TJ06NYvV5C87fiRJkiRJhWFrHT8u9ZK2yo4fSZIkSVJhWLUqeXbGj1rNmjWL8847b5P3unTpwrRp03JUUf4x+JEkSZIkFQY7frSZsWPHMnPmzFyXkddc6iVJkiRJyn9NTVBXB6nUxqAnw+An62KMuS6hJO3Mr7vBjyRJkiQp/2W6fXr2hBA2PWbwk1VVVVWsWLHC8CfLYoysWLGCqqqqHfo+l3pJkiRJkvJfJvjp3fv9xwx+smrIkCEsXLiQZcuW5bqUklNVVcWQIUN26HsMfiRJkiRJ+S+zlfvmg53B4CfLKioq2GOPPXJdhtrJpV6SJEmSpPyXCX42H+wMmwY/Lj+SNmHwI0mSJEnKf9vq+Ckvh6oqSKdh/fqsliXlO4MfSZIkSVL+29pW7hku95K2yOBHkiRJkpT/trXUCwx+pK0w+JEkSZIk5b/tBT81NcmzwY+0CYMfSZIkSVL+yyz12tKMH7DjR9oKgx9JkiRJUn5raEgeFRXQteuWz8kEP7W12atLKgAGP5IkSZKk/NZ2sHMIWz7Hjh9piwx+JEmSJEn5bXvzfcDgR9oKgx9JkiRJUn7LBD9bm+8DDneWtsLgR5IkSZKU39ou9doaO36kLTL4kSRJkiTlN5d6STvN4EeSJEmSlN/s+JF22naDnxBCVQjhmRDCCyGEOSGE77W+v0cIYVoI4fUQwh0hhMrOL1eSJEmSVHJWrUqetzXjp1u3ZMev+npoaclOXVIBaE/HTyPwoRjjgcA44IQQwgTgx8DPYox7AauA8zutSkmSJElS6WpPx08IG7t+6uo6vSSpUGw3+ImJTK9cResjAh8C7mp9/xbg9M4oUJIkSZJUwmJs365e4HIvaQvaNeMnhFAWQpgJLAUeAt4AVscYm1tPWQgM3sr3fiGEMD2EMH3ZsmUdULIkSZIkqWTU10NzM1RVQZcu2z7X4Ed6n3YFPzHGlhjjOGAIcBgwqr03iDFeH2McH2Mc379//52rUpIkSZJUmjLLvHr33v65Bj/S++zQrl4xxtXAI8ARQK8QQnnroSHAoo4tTZIkSZJU8tq7zAsMfqQtaM+uXv1DCL1aX3cFjgNeJgmAPtZ62meAuzupRkmSJElSqcoEP9sa7Jxh8CO9T/n2T2EQcEsIoYwkKLozxvi3EMJLwB9DCP8PeB74bSfWKUmSJEkqRTvT8VNb22nlSIVmu8FPjPFF4KAtvP8mybwfSZIkSZI6R3u2cs+w40d6nx2a8SNJkiRJUla51EvaJQY/kiRJkqT8tSPBT01N8mzwI73H4EeSJEmSlL8yS73c1UvaKQY/kiRJkqT8FOPOBT8Od5beY/AjSZIkScpPtbWQTieBTnk7NqWurEzOa2qCDRs6vz6pABj8SJIkSZLy047M9wEIwTk/0mYMfiRJkiRJ+SkT/LRnmVeGc36kTRj8SJIkSZLyU2a+T3s7fsA5P9JmDH4kSZIkSflpR5d6gR0/0mYMfiRJkiRJ+WlXOn4MfiTA4EeSJEmSlK+c8SPtMoMfSZIkSVJ+cqmXtMsMfiRJkiRJ+cmOH2mXGfxIkiRJkvJPS0uyM1cI0KNH+7/P4EfahMGPJEmSJCn/rF2bPPfsCakd+K1rTU3ybPAjAQY/kiRJkqR8tDPLvMCOH2kzBj+SJEmSpPyzM4OdAaqrk+d16yDGjqxIKkgGP5IkSZKk/LOzHT/l5VBVBek0rF/f4WVJhcbgR5IkSZKUf9asSZ53tOMHXO4ltWHwI0mSJEnKPzu71Asc8Cy1YfAjSZIkSco/uxL82PEjvcfgR5IkSZKUfzJLvXZ0xg9sDH5qazuuHqlAGfxIkiRJkvKPHT9ShzD4kSRJkiTll6YmqK+HsrKN27PvCIMf6T0GP5IkSZKk/NK22yeEHf9+hztL7zH4kSRJkiTll0zwszPzfcCOH6kNgx9JkiRJUn7JDHbemfk+YPAjtWHwI0mSJEnKL7sy2BkMfqQ2DH4kSZIkSfnFjh+pwxj8SJIkSZLyy67O+OnWLRkKXV8PLS0dVpZUiAx+JEmSJEn5ZVeXeoWwseunrq4jKpIKlsGPJEmSJCm/7GrHD7jcS2pl8CNJkiRJyh8x7vqMHzD4kVoZ/EiSJEmS8kdjY/Lo0gWqqnb+OgY/EmDwI0mSJEnKJ22XeYWw89cx+JEAgx9JkiRJUj7Z1cHOGQY/EmDwI0mSJEnKJ5n5Prsy2Bk2Bj+1tbt2HanAGfxIkiRJkvJHR+zoBVBTkzzb8aMSZ/AjSZIkScofmaCmR49du45LvSSgHcFPCGFoCOGREMJLIYQ5IYRLW9//rxDCohDCzNbHSZ1friRJkiSpqK1fnzx37bpr1zH4kQAob8c5zcDXY4wzQgg1wHMhhIdaj/0sxnhl55UnSZIkSSopHR38OONHJW67wU+M8R3gndbXtSGEl4HBnV2YJEmSJKkE2fEjdagdmvETQhgBHARMa33r4hDCiyGEm0IIvbfyPV8IIUwPIUxftmzZrlUrSZIkSSpu9fXJc7duu3adykqoqICmJtiwYdfrkgpUu4OfEEJ34M/AV2OMa4HfACOBcSQdQVdt6ftijNfHGMfHGMf3799/1yuWJEmSJBWvTPCzqx0/Idj1I9HO4CeEUEES+twWY/wLQIzx3RhjS4wxDdwAHNZ5ZUqSJEmSSkJHLfUCgx+J9u3qFYDfAi/HGH/a5v1BbU47A5jd8eVJkiRJkkpKJvjZ1aVe4IBnifbt6nUUcB4wK4Qws/W97wBnhxDGARGYB1zYCfVJkiRJkkpFc3MykyeVSubz7Co7fqR27er1JBC2cOi+ji9HkiRJklSy2nb7hC39NnQHGfxIO7arlyRJkiRJnaYjl3mBwY+EwY8kSZIkKV901I5eGQY/ksGPJEmSJClPdOSOXmDwI2HwI0mSJEnKFx0d/NTUJM8GPyphBj+SJEmSpPyQWerljB+pwxj8SJIkSZLyg0u9pA5n8CNJkiRJyg8dPdy5ujp5XrcOYuyYa0oFxuBHkiRJkpQfOno79/LyJERKpzdeWyoxBj+SJEmSpPzQ0Uu9wOVeKnkGP5IkSZKk/NDRw53B4Eclz+BHkiRJkpQf7PiROpzBjyRJkiQpP3Rm8FNb23HXlAqIwY8kSZIkKT90xlKvmprk2Y4flSiDH0mSJElSfnCpl9ThDH4kSZIkSbkXIzQ0JK+rqjruugY/KnEGP5IkSZKk3GtoSMKfqipIdeBvVQ1+VOIMfiRJkiRJuZeZ79ORy7zA4Eclz+BHkiRJkpR7mfk+HTnYGQx+VPIMfiRJkiRJudcZg53B4Eclz+BHkiRJkpR7nbGVe+Z6ISTXT6c79tpSATD4kSRJkiTlXmd1/ISw8ZqZcEkqIQY/kiRJkqTc66zgBzZ2ERn8qAQZ/EiSJEmScq+zlnoBVFdveg+phBj8SJIkSZJyz44fqVMY/EiSJEmSci8TynRG8OOMH5Uwgx9JkiRJUu51ZsePS71Uwgx+JEmSJEm5lwl+OmPGj0u9VMIMfiRJkiRJuWfwI3UKgx9JkiRJUu515owfgx+VMIMfSZIkSVLuZWNXr7q6jr+2lOcMfiRJkiRJuRWj27lLncTgR5IkSZKUW83NyaO8HCoqOv76meAnEy5JJcTgR5IkSZKUW5052LntdV3qpRJk8CNJkiRJyq3OHOwMUF296X2kEmLwI0mSJEnKrc4OfjLXXb8+mScklRCDH0mSJElSbnXmYGeAVAqqqpLQp6Ghc+4h5SmDH0mSJElSbnX2jJ+213a5l0qMwY8kSZIkKbc6u+MHDH5Usgx+JEmSJEm5lQljstHx485eKjEGP5IkSZKk3LLjR+o02w1+QghDQwiPhBBeCiHMCSFc2vp+nxDCQyGE11qfe3d+uZIkSZKkopPN4CdzL6lEtKfjpxn4eoxxP2AC8OUQwn7At4CHY4x7Aw+3fi1JkiRJ0o5xqZfUabYb/MQY34kxzmh9XQu8DAwGTgNuaT3tFuD0TqpRkiRJklTMstHxU12dPLvUSyVmh2b8hBBGAAcB04CBMcZ3Wg8tAQZu5Xu+EEKYHkKYvmzZsl2pVZIkSZJUjDJhjDN+pA7X7uAnhNAd+DPw1Rjj2rbHYowRiFv6vhjj9THG8THG8f3799+lYiVJkiRJRSjT8dOZS70yoZLBj0pMu4KfEEIFSehzW4zxL61vvxtCGNR6fBCwtHNKlCRJkiQVNZd6SZ2mPbt6BeC3wMsxxp+2OXQP8JnW158B7u748iRJkiRJRS+bw50NflRiyttxzlHAecCsEMLM1ve+A/wIuDOEcD7wNvCJTqlQkiRJklS80mlobIQQoEuXzruPwY9K1HaDnxjjk0DYyuFjOrYcSZIkSVJJabvMK2ztt54dwOBHJWqHdvWSJEmSJKlDZWO+D2wMfurqIG5xbyKpKBn8SJIkSZJyJ1vBT3k5VFQkS8s2bOjce0l5xOBHkiRJkpQ72RjsnOHOXipBBj+SJEmSpNzJVscPOOdHJcngR5IkSZKUO5kQJhvBT+YeBj8qIQY/kiRJkqTcyXT8uNRL6hQGP5IkSZKk3HGpl9SpDH4kSZIkSbmTzeHOBj8qQQY/kiRJkqTcseNH6lQGP5IkSZKk3MlF8FNX1/n3kvKEwY8kSZIkKXeyOdzZjh+VIIMfSZIkSVLuZHM790zwkwmbpBJg8CNJkiRJyp1cBD8u9VIJMfiRJEmSJOVONmf8VFcnzy71Ugkx+JEkSZIk5UaM2Q1+Mvcw+FEJMfiRJEmSJOXGhg2QTkNFBZSXd/79HO6sEmTwI0mSJEnKjWzu6AVQWQllZdDUlDykEmDwI0mSJEnKjUznTbaCnxDc2Uslx+BHkiRJkpQb2Zzvk+HOXioxBj+SJEmSpNzIZfDjnB+VCIMfSZIkSVJuGPxInc7gR5IkSZKUG9me8dP2XgY/KhEGP5IkSZKk3MiEL9ns+Kmu3vTeUpEz+JEkSZIk5UYulnpl7mXwoxJh8CNJkiRJyo1M8ONSL6nTGPxIkiRJknIjFx0/LvVSiTH4kSRJkiTlhsOdpU5n8CNJkiRJyg23c5c6ncGPJEmSJCk3DH6kTmfwI0mSJEnKjVwu9aqry949pRwy+JEkSZIk5UYuO34y95aKnMGPJEmSJCn7mpthwwZIpaCyMnv3raqCEKChAdLp7N1XyhGDH0mSJElS9rXt9gkhe/cNwTk/KikGP5IkSZKk7MvFMq+MzD0NflQCDH4kSZIkSdmXCX6yOdg5o7o6eTb4UQkw+JEkSZIkZV8uO35c6qUSYvAjSZIkScq+XGzlnmHwoxJi8CNJkiRJyj47fqSsMPiRJEmSJGVfPgQ/dXXZv7eUZdsNfkIIN4UQloYQZrd5779CCItCCDNbHyd1bpmSJEmSpKLiUi8pK9rT8XMzcMIW3v9ZjHFc6+O+ji1LkiRJklTU8qHjJ1ODVMS2G/zEGB8HVmahFkmSJElSqch027jUS+pUuzLj5+IQwoutS8F6b+2kEMIXQgjTQwjTly1btgu3kyRJkiQVjXzo+HGpl0rAzgY/vwFGAuOAd4CrtnZijPH6GOP4GOP4/v377+TtJEmSJElFJRP8OONH6lQ7FfzEGN+NMbbEGNPADcBhHVuWJEmSJKmoOdxZyoqdCn5CCIPafHkGMHtr50qSJEmS9D65XOpVXZ08G/yoBJRv74QQwh+AyUC/EMJC4P8Ak0MI44AIzAMu7LwSJUmSJElFJ5fBT+ae69dDjBBC9muQsmS7wU+M8ewtvP3bTqhFkiRJklQKYsxt8JNKQVUVNDQkdeRiuZmUJbuyq5ckSZIkSTuusTEJf7p0SUKYXHDOj0qEwY8kSZIkKbtyOdg5w+BHJcLgR5IkSZKUXblc5pVh8KMSYfAjSZIkScquTNiSy+DHnb1UIgx+JEmSJEnZlQ8dP5l7G/yoyBn8SJIkSZKyKxP8OONH6nQGP5IkSZKk7MqH4c4u9VKJMPiRJEmSJGVXPiz1suNHJcLgR5IkSZKUXfkQ/DjjRyXC4EeSJEmSlF35EPy41EslwuBHkiRJkpRd+TDjJ3Pvurrc1SBlgcGPJEmSJCm7MsGPM36kTmfwI0mSJEnKrnxY6pUJfjK1SEXK4EeSJEmSlF2ZsCVflnrFmLs6pE5m8CNJkiRJyq586PgpL4eKCkinYcOG3NUhdTKDH0mSJElSduXDcGdwZy+VBIMfSZIkSVL2NDVBc3PScVNentta3NlLJcDgR5IkSZKUPW2XeYWQ21oc8KwSYPAjSZIkScqefJjvk+GW7ioBBj+SJEmSpOzJx+DHpV4qYgY/kiRJkqTsyZfBzm1rsONHRczgR5IkSZKUPfnY8eOMHxUxgx9JkiRJUvZkumvyKfhxqZeKmMGPJEmSJCl7Mt01LvWSssLgR5IkSZKUPfm41MvgR0XM4EeSJEmSlD0Od5ayyuBHkiRJkpQ9+dTxU12dPBv8qIgZ/EiSJEmSsiefgh87flQCDH4kSZIkSdmTT0u9MuGTwY+KmMGPJEmSJCl78qnjp7ISysqgqSl5SEXI4EeSJEmSlD2Z7pp8CH5CcLmXip7BjyRJkiQpe/Kp4wcMflT0DH4kSZIkSdmRTkNDQ9JpY/AjZYXBjyRJkiQpOxoakueqqiT8yQcGPypyBj+SJEmSpOzIt2VeYPCjomfwI0mSJEnKjnzayj2jujp5NvhRkTL4kSRJkiRlhx0/UtYZ/EiSJEmSsiMfg59MLQY/KlIGP5IkSZKk7HCpl5R12w1+Qgg3hRCWhhBmt3mvTwjhoRDCa63PvTu3TEmSJElSwcvHjp9MCFVXl9s6pE7Sno6fm4ETNnvvW8DDMca9gYdbv5YkSZIkaesyXTX5GPzY8aMitd3gJ8b4OLBys7dPA25pfX0LcHrHliVJkiRJKjr53PGTqU0qMjs742dgjPGd1tdLgIFbOzGE8IUQwvQQwvRly5bt5O0kSZIkSQUvE67k04wfl3qpyO3ycOcYYwTiNo5fH2McH2Mc379//129nSRJkiSpUOXjcGeXeqnI7Wzw824IYRBA6/PSjitJkiRJklSU8nGpV1UVhACNjdDSkutqpA63s8HPPcBnWl9/Bri7Y8qRJEmSJBWtfAx+QrDrR0WtPdu5/wGYCuwbQlgYQjgf+BFwXAjhNeDY1q8lSZIkSdq6fAx+wAHPKmrl2zshxnj2Vg4d08G1SJIkSZKKWT7O+AE7flTUdnm4syRJkiRJ2xVj/nf8uLOXipDBjyRJkiSp8zU1JcOTKyqgfLuLT7LLjh8VsTz7tEmSJElS/mhpgdrapFmld+9cV1PgMqFKvnX7gMGPiprBjyRJkqSSVFcHixbB2rWwZs3GR+brtWth3bok9AHYfXc45BA4+ODktXZQvi7zAoMfFTWDH0mSJEklpbYWHnwQHn00WX20LSFAjx6wYQMsXpw87r0XBg1KAqCDD4bBg5PztB2Z4CffBjuDwY+KmsGPJEmSpJKwbh384x/wyCNJkAMwYgT06QM9eyYBT8+eGx89ekBNDaRS0NwMr74Kzz0HM2fCO+/A3/+ePAYM2NgJNHSoIdBW2fEj5YTBjyRJkqSiVle3MfBpbEzeO+AAOOUUGD68fdcoL4cxY5LHOefA3LkwYwY8/zwsXQr33588+vWD00+HQw/ttB+ncOXrVu5g8KOiZvAjSZIkqSjV1cE//wkPP7wx8Nl/f/jIR5JOn51VVgajRyePs8+G119POoFmzIDly+HGG2HWLPjkJ/Mz48iZfO74qa5Ong1+VIQMfiRJkiQVlfr6JOz55z+hoSF5b8yYJPDZY4+OvVcqBfvskzzOOguefBL+9CeYNg1eew0+9znYe++OvWfByufgJ1OTwY+KkMGPJEmSpKLx8stwww1Jtw8kXTmnngp77tn5906l4AMfgH33hZtugnnz4Kqr4Pjjk9CpvNR/9+VSLyknSv1fPZIkSZKKxGOPwR//COl00mVz+umw1147d610TDNv9TxmvTuLWUtnsbh2MeWpcipSFVSUVVBZVvne67bvdSnrwn799+PSr43joQe6cP/98MADMGcOnH9+shtYycrnjh+XeqmIGfxIkiRJKmjpNNx5ZzK8GeDEE+G003Z8d636pnpeWvYSs96dxeyls1m3Yd0mx1vSLTTSuN3rPL3waSrLKhk3dBynff5wnvjLfixYkOK//xs+9jH44AdLdOevTKiSj8FPpqb16yHGEv0bpGJl8CNJkiSpYK1fnyztmjMnGbr86U/DhAnt+94YI0vWLWHW0lnMencWr698nXRMv3e8X7d+jB04lrEDxrJXn72IRJpamtjQsoHmdDMbWjbQlG6iqaWJpnTy/uqG1UxfPJ03Vr7BM4ueAZ6h61E1pN4az4o5h3P7H0Ywa1bgM59JtosvKZmOn3xc6pVKQVVVMhRq/fr8rFHaSQY/kiRJkgrS8uVw9dXwzjvQvTt86UvtX9o1d8Vc7pxzJwvWLHjvvVRIsU/ffd4Le3brvhths86PqvKq7V77Q3t8iOX1y3lm0TNMWziNJeuWwLBHaOz2CG/OHcD81w7jpe8fzsWfHcCYMTv0Ixe21auT55qanJaxVd26JcFPfb3Bj4qKwY8kSZKkgvP66/Cb38C6dcncnIsvhn79tv99K9ev5M8v/Znpi6cDUF1ZzdgBYxk7cCz79d+PbhUd8xv+ft36cdLeJ3HiXicyf818nln0DM90eYYePZYyd+7feHTV35h90wH830+czeTD+3TIPfNajPDuu8nr3XbLbS1b060brFzpnB8VHYMfSZIkSQXl6afh1luhuTnZpv3zn9/+2JimliYefONBHnj9AZpamqgoq+CkvU/iuD2Po6KsotNqDSEwvNdwhvcazpn7ncmry1/l6T2n8acnZ/DWghe55M+v8qVVp/HF448mFVKdVkfOrVwJTU3Qs2eypCofubOXipTBjyRJkqSCECPcfTfcf3/y9dFHwyc+kYxn2fr3RGYumcmfXvoTK+pXADB+9/Gcud+Z9Oma3U6bVEgxuv9oRvcfzRmjzuDbt/+RR1+dwdX/upPZq57hOyefx5AeQ7JaU9YsWZI8DxyY2zq2xZ29VKQMfiRJkiTlvQ0b4Kab4Pnnk6DnrLNg8uRtf8/i2sXcMfsOXln+CgBDegzhrP3PYp+++3R+wdvRq2tPrvnchfzqzhe46dnbefT5eSxZ/998+sjjOXnvkzu1Cykn8n2ZF9jxo6Jl8CNJkiQprzU3w7XXJjt3VVXBhRfCfvtt/fz6pnruffVeHp33KOmYprqymtP2PY1Jwyfl1XKqEOArZx3IsOp9+dU//8orrzzG/6Tv57nFz3HuAeeyb799c11ix8l0/ORz8JNZL2jwoyJj8CNJkiQpb6XT8NvfJqFPTQ187Wuw++5bP/+V5a9w44wbqW2sJYTA5BGTOXXfU6murM5e0Tvo9FOq6FpxNjfffTivvXwrMS5mad1POXLokXxsv4/lde3tlun4camXlHUGP5IkSZLyUozJEOcZM5JmjEsv3XroE2PkX2/9i7teuot0TLNXn704e+zZBTMz5/jjoaxsT+7403dZOOsfvBv/zhSmMGvpLD419lMcPOjgXJe4awqh4yez1KuuLrd1SB3M4EeSJElS3okR7rwTpkyBykq45BIYOnTL5za1NPH7F3/P0wufBuDEvU/k1H1PzatlXe1x7LFQVlbOH/94EvXPHwJdbqW2z2tcN/06TtjrBE4fdTohhFyXueMaGmD1aigvhz55vHW9M35UpArr34SSJEmSSsK998K//pVkBRddBCNHbvm8VetXccWUK3h64dNUllXy+UM+z+mjTi+40Cfj6KPhnHOgW3ogccrX2XP9WaRCigdef4DrnruOxubGXJe449ou89rWFmy5lgl+1q/PbR1SB7PjR5IkSVJeeegh+Pvfk4zgggtg9Ogtn/f6yte5dvq11DbW0q9bP7506JcKZmnXtnzgA8nP/vvfB958+EMcfPwgXu52Hc+/8zzL65fz5UO/TO+uvXNdZvsVwlbu4FIvFa08jlslSZIklZonnoC77kpef+YzcNBBWz7v8bcf56opV1HbWMuofqP4zqTvFEXokzFxYvLzhwAzHhzNB8u+Rf/q/ixYs4AfPvlD3l79dq5LbL9C2ModXOqlomXwI0mSJCkvPPss3HZb8vrss2HChPef05xu5vcv/p7bXryNdExz7J7HcumES4tj56vNHHEEnHtu8vrBP+/GKb2+zT5992FNwxqumHIFM96ZkdsC26sQBjuDwY+KlsGPJEmSpJybNQtuuikZ6nz66TB58vvPWdOwhp9O/SlPvP0EFWUVfO6gz/HxMR8v2Hk+7TFxIpxySvLr8vubqjl14KUcNewomlqauG76ddz32n3EGHNd5rYVwlbusGnwk++/ptIOKN5/Q0qSJEkqCHPnwnXXQTqdbGt+wgnvP2fBmgX84Ikf8MbKN+jdtTffOPIbHD7k8OwXmwOnnAJHHQVNTXDtNeUcv9t5fGy/jxFC4O5X7uZ/Zv4PTS1NuS5zy2IsnKVe5eVQUZH8g7hhQ66rkTqMwY8kSZKknJk3D66+Ogk1PvABOOOMZK5NW6+teI0rp1zJ6obV7N13b7476bsM7zU8J/XmQgjJTl9jxsC6dfCrXwUmDDiOL43/El3KuzBt4TR+OvWnrG1cm+tS32/lyuRvbs+eUFWV62q2r7p1yaDLvVREDH4kSZIk5cSyZUno09gIhx2WzPXZPPSZs3QOv5j2CxqaGxi/+3i+OuGr1HSpyU3BOVRWBhdeCMOGbfx1G9X7QL551Dfp3bU3b656kx89+SPeXfdurkvdVKHs6JXhzl4qQgY/kiRJkrKurg5+9SuorYX99oN/+7dkC/O2nlv8HL9+9tc0tTQxcdhEzj/4fMpT5TmpNx906QKXXAJ9+yadUjfcALt3H8J3Jn2HPXrvwYr6FVwx5QoWrl2Y61I3KpTBzhmZjp/a2tzWIXUggx9JkiRJWdXcDL/5TTL6ZciQpJOlrGzTc56c/yQ3zLiBlnQLx408jnMPOLeohzi3V48ecOmlST4xaxbcfjvUVPbgsgmXMbr/aGoba7lqylXMWz0v16UmCmW+T8agQcnzokW5rUPqQP6bU5IkSVLWxAg33wyvvQa9esHFF79/9MtDbzzErS/cSoyR00adxpmjzyRsvgashA0cmPy6VVTAE0/AffdBl/IuXHzYxRy424HUN9Xz06k/5bUVr+W61MLr+Bk2LHmePz+3dUgdyOBHkiRJUtbccw88+2yybOnii6F3743HYozc/crd3PXSXQCcPfZsTtr7JEOfLdhzT7jggmQm0j33wJQpUJ4q58JDLuSwwYfR2NzIL6b9gjlL5+S20ELZyj3D4EdFyOBHkiRJUlY89VTSnZJKwRe+AEOHbjwWY+SOOXdw32v3kQopPnvQZ5k8YnLOai0E48bBJz+ZvL71VpgzB8pSZXz2oM8ycdhEmlqa+PWzv+b5d57PTYENDbB6dbJNep8+ualhR+2+e/IP6JIlydRxqQgY/EiSJEnqdC+9BL//ffL6U5+C/fffeCwd09w882YeeeuRpGtl/IVMGDIhN4UWmMmT4fjjIZ2G665LGlVSIcW5B5zLMXseQ0u6heufu55pC6dlv7i23T6bT+7OVxUVSfgTIyzMoyHZ0i4okE+fJEmSpEK1aFESSqTTSUgxadLGY00tTVw7/VqeXvg0Xcq7cMnhlzBut3E5q7UQnXEGHH540qDyq1/BypUQQuDj+32ck/c5mXRM8z8z/4cn3n4iu4UV2lbuGS73UpEx+JEkSZLUaVavTsKIhgYYPz4JKTI2tGzg18/+mheWvEC3im58dcJXGdVvVM5qLVQhwKc/DfvsA2vXJr/e69cn4c+p+57KR0d/lBgjv3/x9/zzzX9mr7BCG+ycYfCjImPwI0mSJKlTNDTA1VfDqlUwciT8278lIQVAY3Mjv5r2K15e9jI1XWr4+pFfZ8/ee+a03kJWXg5f+lKyG/nixXDttdDcnBw7fq/jOXvs2QD8ac6f+NvcvxFj7PyiCm0r9wyDHxWZXQp+QgjzQgizQggzQwjTO6ooSZIkSYUtnYYbboAFC2DAALjoomR8CkBDcwO/mPYL5q6YS8+qnlx+5OUM6TEktwUXgW7d4JJLoEcPeOWVZKZSJt+ZPGIy/zbu3wghcO+r9/LXV/7a+eFPoS71GjIkSSgXL96YnkkFrCM6fo6OMY6LMY7vgGtJkiRJKnAxwh/+ALNnQ3V1EkZ0754cq2+q5+dP/5w3Vr5B7669ufzIy9mte4F1hOSxvn3h4ouhshKmToW//33jsSOGHsHnD/48qZDiwdcf5E8v/anzwp8YYenS5HWhdfx06ZKEVel0MqBKKnAu9ZIkSZLUoR54AB5/PFl+9OUvJx0/AHUb6vjZ1J/x1qq36NutL5cfeTkDqgfkttgiNHw4fP7zSdPKvfcmAVDGIbsfwhfHf5GyVBkPv/kwf5j9h84Jf1auhKYm6NkTqqo6/vqdzeVeKiK7GvxE4B8hhOdCCF/Y0gkhhC+EEKaHEKYvW7ZsF28nSZIkKZ9NmQL/+79J6HD++clsH4Daxlp+OvWnzF8zn/7V/bn8yMvp161fTmstZgccAJ/8ZPL6d79Lln5lHLjbgVx06EWUp8p5bN5j3PriraRjumMLKNTBzhkGPyoiuxr8TIwxHgycCHw5hPCBzU+IMV4fYxwfYxzfv3//XbydJEmSpHw1Zw7cemvy+qyz4OCDk9drG9dy1dSrWLh2IQO7D+TyIy+nT9c+uSu0REyeDMcdl6xYuvbaZGRNxv4D9ueSwy+hoqyCp+Y/xc0zb+7Y8KdQ5/tkGPyoiOxS8BNjXNT6vBT4K3BYRxQlSZIkqbC8/TZcd10SMhx/PBx9dPL+6obVXDnlSt6pfYdBNYO4/MjL6VXVK6e1lpIzz0wCuPXrk23e16zZeGxUv1F85fCv0KW8C9MWTuO3M35LS7qlY25c6B0/Q4cmzwsXQksH/ZpIObLTwU8IoTqEUJN5DXwYmN1RhUmSJEkqDMuWJaFCYyMcfjiccUby/sr1K7lyypW8u+5dhvQYwteP+Do9uvTIbbElJgT43Odgzz2TsTtXX538fcrYp+8+XHr4pVSVVzF98XRumHEDzekO2MmqULdyz+jWDfr1S3b1yoRYUoHalY6fgcCTIYQXgGeAv8cYH+iYsiRJkiQVgtpa+OUvk+fRo+HTn07ChuX1y7lyypUsq1vGsJ7D+NoRX6OmS02uyy1JFRVw0UXQv3+ycumGG5LOrIyRfUZy2RGX0a2iG8+/8zzXTr+WppamXbtpJvgp1KVe4HIvFY2dDn5ijG/GGA9sfYyJMf53RxYmSZIkKb81NiYdJEuXJitjvvjFZCevJeuWcOWUK1lRv4IRvUZw2RGXUV1ZnetyS1pNDXzlK1BdDbNmwe9/n+y4njGi1wi+dsTXqK6sZta7s7jm2WvY0LJh527W0ACrVyeJU58CnuVk8KMi4XbukiRJknZYOp10jsybB337JqFCVRXMXzOfK566glXrV23SSaLcGzAAvvzlJI956im4665Nw5+hPYdy+ZGXU9OlhpeWvcTVz1xNY3Pj1i+4NZlunwEDIFXAv+U0+FGRKOBPoSRJkqRciBFuuy3pHKmuhksvhR494LUVr3HVlKtYt2EdYwaM4asTvkpVeVWuy1UbI0fCl74EZWXwz3/C3/++6fHda3bn8iMvp2dVT15d/io/f/rn1G2o27GbFPpg54xM8LNgwaYJmVRgDH4kSZIk7ZC//Q2efDLpHLn44mSMy6x3Z/GLab+gobmB8buP56JDL6KyrDLXpWoLxoyBCy5IZjHdey88/PCmx3frvhuXH3k5fbr24c1Vb3LllCtZ3bC6/Tco9K3cM2pqoHfvZE3j0qW5rkbaaQY/kiRJktrtiSeS4CcE+Pznk92inln0DNc8ew1NLU1MGj6J8w8+n/JUea5L1TYcfHAyiBvgzjuTpV9tDagewDeP+iaDagaxuHYxP37yx7y77t32XbxYOn7A5V4qCgY/kiRJktpl+vRkiRfApz4FBx4Ij857lJuev4l0THP8XsdzzthzSAV/m1EIjjwSzjoreX3rrfDcc5se7921N9848hvs2XtPVq5fyU+e+glvr357+xcu9K3c2zL4URHw38iSJEmStmv6dLjxxmTUyUc+ApMmRe577T7+MOsPxBj56OiP8tHRHyWEkOtStQM+9CE49dTk7+tvfwuzZ296vLqymq9O+Cr7D9ifdRvWcdXUq3h52ctbv2A6vXFZVKEv9YJkuzow+FFBM/iRJEmStE1tQ59TToGTTor8+eU/c/crdxNC4NwDzuX4vY7PdZnaSSedBMceCy0tcO218Nprmx7vUt6Fiw69iMOHHE5jcyNXP3M1zy1+bssXW7kSmpqgV69km7dC17bjxwHPKlAGP5IkSZK26n2hz8lpfj/rVh564yHKUmVccPAFTBo+KddlaheEAB/7GEycmGQ2V18Nb2+2oqssVcZnx32WY/Y8huZ0MzfMuIHH5j32/otllnkVQ7cPJAFWTQ3U1yehllSADH4kSZIkbdHmoc/xJzZx44wbeGr+U1SUVfDlQ7/M+N3H57pMdYAQ4JxzYPx4aGiAX/wC3nln83MCH9/v45w+6nRijNw+63b+NvdvxLadMMWyo1dGCM75UcEz+JEkSZL0PpuHPpOOXcNVU69kxjsz6FrRlcsmXMaYAWNyXaY6UCoFn/0s7L8/1NXBz38Oy5dvek4IgRP3PpHzDjyPEAL3vnovf5z9x43hTzHt6JVh8KMCZ/AjSZIkaRObhz5jJ77ND5/8AfNWz6Nvt75848hvMLLPyFyXqU5QXg5f/CLsvTesXg1XXgmLF7//vInDJnLhIRdSnirn0XmPcsOMG9jQsqG4dvTKMPhRgTP4kSRJkvSezUOf3Q6ezpVTr2B1w2r26rMX3574bQb3GJzrMtWJKirg4othr71g1Sq44gp44433n3fQoIO4dMKlVJVX8dzi5/jJUz9hxZK3koPFGPy8/bYDnlWQDH4kSZIkAZuGPiefHIl738uNM26gqaWJo4YdxWVHXEZNl5pcl6ksqKqCr34VDjwwmWv8s5/Biy++/7x9+u7Dv0/8dwZUD2DBynn8ID7Gq1XroE+frNfcafr2ha5dobYW1qzJdTXSDjP4kSRJkrRJ6HP8SY0s3v16/v7a3wgh8Ikxn+C8A86jPFWe6zKVRRUVybKvo45Kdvv6zW9gypT3n7d7ze58e9K3GdNlMOtCEz/vM5eH3/rXpkOfC5kDnlXgDH4kSZKkEvfEExtDn8knrGJOryt4/p0ZVJVXcclhl3DMnscQQsh1mcqBVArOOw9OOgnSabjlFnjwwfeveOpW0Y2Le5/ACY1DSXfryp1z7uTmmTfT1NKUm8I7msGPCpiRvSRJklSi0mn405/gX/9Kvh5/3Js8V/0bateuZUD1AC469CIG1QzKbZHKuRDgtNOgRw+44w74y1+SFU8f/3hyLCP17lLOaNiDYUP24eayeTy98GkW1y7mS4d+iT5dC3zpVyb4WbAgt3VIO8GOH0mSJKkE1dfDL3+ZhD5lZXDIaU8zs+tV1DauZVS/UXxr4rcMfbSJo4+GCy5I/nl5+GG46SZobm5zQutW7ofsOZF/n/jv9OvWj/lr5vODJ37A3BVzc1N0R7HjRwXM4EeSJEkqMUuWwA9/CC+/DF1rGtnro7fxXMv/0JxuZvKIyXzl8K9QXVmd6zKVh8aPh0sugS5d4Jln4Ne/hsbG1oNttnIf0mMI35n0HUb3H01tYy0/m/ozHnnrkcKd+zNgQPJDr1wJ69bluhpphxj8SJIkSSVkzhz40Y9g6VLoNvhNyo7+v7y6/nHKU+WcPfZszh57NmWpslyXqTw2ejR8/etQUwMvvQQ//SnUrklvDH4GDgSgurKarxz+FT488sOkY5o/zv4jt7xwC43Njdu4ep5KpWDIkOS1y71UYAx+JEmSpBIQI/zzn/CrX0Hd+mZS+/0v6w78CevSyxjcYzDfnvRtJo+YnOsyVSCGD4dvfhP69YN58+BH/9XAvFU9oVevZC/4VqmQ4sz9zuSCgy+goqyCqQum8v3Hvs8ry1/JWe07zeVeKlAGP5IkSVKRa26G3/0uGeS8Lixm/SE/omXP+ykrg+P3Op7vTPoOQ3oMyXWZKjADBsC//3uShyxf2MCPZx7Pg6sOe9+OXwCHDj6Ub038FkN6DGF5/XJ+NvVn3PbibTQ0N2S/8J1l8KMCZfAjSZIkFbG1a5OlOE9NiSzp9k82HP7fVA9aQP9u/bj8yMv56OiPUp5ys1/tnB49ks6fY0YtIh0Df3njAH7+c1i9+v3nZub+nLrvqZSlynj87cf53qPf4+VlL2e77J1j8KMCZfAjSZIkFam334Yf/ADmvLmC1/v/lO6H/YnefZuZOGwi/98H/z/26rNXrktUEaiogE+MepFL9n+Emr6VvPIKfP/78MIL7z+3LFXGyfuczHcnfZfhvYazcv1Kfv70z7n1hVtZ37Q++8XviEGDoLw8GZC1Ps9rldow+JEkSZKKTGMj3HUX/OCHkVfWTeX1Id9n+MFzGdS3hi8f9mXOO/A8qsqrtn8hqb2WLGH/Pov5z6/XM2YM1NXBNdfA7bfDhg3vP31wj8F8a+K3OH3U6ZSnynly/pN877HvMXvp7OzX3l5lZTB4cPLaAc8qIPZ0SpIkSUVkzhy47TZ4e9VC5nX7M92Gv8R+I+CQwQdxzthzqOlSk+sSVYxad/TqsdcALjkM/vUv+Mtf4LHHYO5cuOCCjZtiZaRCihP3PpFxu43jlhdu4a1Vb/Grab/iyKFH8vExH6dbRbcc/CDbMWxY0ko3fz7ss0+uq5HaxeBHkiRJKgK1tXDnnfD4syt4u+oe6nabxt77RAb07spZY85iwpAJhBByXaaKUUMDrFmTrPnq04cQ4JhjklzkxhvhnXfghz+EM8+Eo4+Gzf8xHFQziG8e9U3++eY/ufuVu5myYApzls3hjFFncPiQw0mFPFqo4pwfFSCDH0mSJKmAxQhTp8If7qrjlZb7WdLrEYYNb2b00DKO3mMyJ+19Et0ru+e6TBWzJUuS54EDN0l1hg6F73432U3u8cfhjjuSjrSzzkp2BGsrFVJ8eOSHOXDggdzywi28sfINbp55M/e/fj8f2ecjjN99fH4El5ngx6VeKiAGP5IkSVKBWroUfvf7Jv41718s6PIANf3rOWgv+ODIwzht1Gn069Yv1yWqFLQNfjZTWQnnnAP77Qe33gqzZ8NLL8HEiXDKKdCz56bnD+w+kMuPvJxnFj3Dva/ey7vr3uXGGTe+FwCN221cbgOgwYMhlUramDZsSH5AKc8Z/EiSJEkFpqUFHvxHmt8++DRvlN9DuvsqRo6ED+43mjP3+yjDeg7LdYkqJa3zfdhtt62ectBBsMcecM89MGVK0gE0dSoceyx8+MPQrc04n1RIMWHIBA7d/VCmLJjC31/7O4vWLuLa6dcyvNdwTt33VMb0H5ObAKiiAnbfHRYuTB577pn9GqQdZPAjSZIkFYimJpj6dJpbH3qB5+vuoa5yMQMHwsQDhvCpcWeyX//9cl2iStE2On7a6tULPv1pOO44uPtueP55uP/+ZAD0iScm838qKjaeX5YqY9LwSRwx9AieePsJ7nvtPt5e/Ta/mvYr9uy9J6eNOo19++6b/QBo2LAk9Jk/3+BHBcHgR5IkScpz9fXwt4dX8cennuTN5idpTK2mqhomjenL+UedxmGDD8uP+ScqTZngZxsdP20NGgRf/CK89Vay89fcufDnP8PDD8NHPgJHHpmspsooT5Vz9B5Hc9Swo3hs3mM88PoDvLnqTX429Wfs03cfTtjrBEb3H529IdDDhiVtSw54VoEw+JEkSZLy1LLlaW657yX+NutxloYXiUS618DBIwfwyQmTOXqPD1BRVrH9C0md5eWX293xs7k99oCvfS2Z+fPXvybzkm+9Ff7xDzj99GR5WNs8s7KskuNGHsek4ZN45K1H+Mcb/2DuirnMXTGXXlW9OGLoERwx5AgGdt+xOnaYO3upwIQYY9ZuNn78+Dh9+vSs3U+SJEkqRC+9sZbr7n+KJ95+gvVhBQB9e5fx4bEH8fHDJzGqXw6Wt0ibe+kluOaaZA3i0UfDJz+505eKEaZPT5aALVuWvNenD0yYkDy2lCnVN9Xz2LzHmLJgCkvrlr73/sg+Izly6JGM3308VeVVO13TVjU2wqWXJm1Jv/wllNtPodwLITwXYxy/xWMGP5IkSVLuNTY1c/+zr3L7E08ye8VMImlCgD0H9uOsIyZx2sFH0qNLj1yXKSXmzElCn+Zm+MAH4FOf2rQ9Zyc1N8NTT8GDD8KKFRvf33NPOOIIGD9+00HQADFG3lj1BlMWTGH64uk0NjcCUFFWwcGDDubIoUd2/Cyg//zPZKj1d7+7sQNIyiGDH0mSJCkPLVq1gr8/M4fHXp7NnHdfobEl+Q1reVmKCSMO5LMfmsQRe+1nd4/yy+zZ8JvfJCnN5MlJp08H/zMaI7z2WrLz13PPJU02kDTXjBuXhED77bfpLCCAxuZGZrwzgykLpjB3xdz33u/brS/jdx/Pfv33Y2Tvkbu+RPLGG+HZZ+GEE5J1aX5GlWMGP5IkSVIeaE4389y813jgudlMfXM2C1YuId3mf8d36zaYo/c9hAuOP4rd+/TKWZ3SVr34Ilx3XRL6HH00nHVWp4cejY3JDmBTp8KrryahEECPHnDYYcksoBEj3r/ialndMqYunMrUBVNZuX7le+9XlFUwsvdIRvcfzah+oxjWc9iOD4Z+9tkk/AEYMybZrqxXr53+GaVdZfAjSZIk5UBTSxOL1i7m2dff4pHZc3h+4Susrt3w3vFyqti712gm7rM/pxw+hn2G9s5htdJ2vPgiXHsttLTAMcfAxz+e9U6XlSth2rQkBHr33Y3vl5cny8H23hv22ScZHN2lS3IsxsirK15l9tLZvLL8FRasWbDJNbtVdGPffvsyqt8oRvcbzYDqAe3rsps+HW6/HerqkvVnn/xkkkTZ/aMcMPiRJEmSOll9Uz3zVy/khXnzmT1/Aa++O58Fq5awbl2a5paN5/VgCAcO2p8P7T+GEw4bSa+eZbkrWmqvF15IOn1aWuDYY+FjH8tpwBFjsh38M88kXUCLF296PJVKuoD23jt57LUXdO2aHKttrOXVFa/yyvJXeHnZyyyvX77J9/as6smQHkMYXDOYwT0GM7hmMLt1323Ly8PWrEm2Ips1K/n6oIPgnHOgpqbjf2hpGwx+JEmSpA4QY2Rt41pWrF/B0tqVzF28lNnz5/PauwtYvGY5dXXJ74szAoGu6d0YUDmUQ4aP4viDx3D4gb2orMzdzyDtsOefh+uvh3QajjsOzjwz77pa6urg9ddh7txkNtD8+RuXhEFS7m67JbuDbf5oSC3nleUv88ryV3hl+Sus27DufddPhRQDqgcwuMdgdq/Z/b1QqE/XPpSHMpgyBe68ExoaktDnnHOSEEjKkk4LfkIIJwC/AMqAG2OMP9rW+QY/kiRJylcxRuqb6qlrqmPV+tUsXLGS+ctWsHDlCt5ZvZJ3a1ewon4l9Q3NNDbAhqZNvz9FBdUtgxlYNZS9Bgxl/6FDOXivIYwcUUkPN+NSoZoxA264IQl9jj8ezjgj70KfLWlogDfeSEKguXNh3rxNQ9m2unbdGAINGBAp77mM+vJF1LKI1elFLG9cxLL6pWzt9841XWroXdWb3ulKek97gd6LVtI73YXeYw+l95nn0Kv37rs+TFrajk4JfkIIZcBc4DhgIfAscHaM8aWtfY/BjyRJkjpajJHmdDMbWjbQ2NJIQ1Mj65s2sG59I/WNG6jfsIG6hkbqGxtZXV/Pmro6VtevY21DHWsb6qhtXMe6xjrqm+ppao40N8OGDcnvc7ekIlbTJd2XrrEv/av7sdeAIRwwfBgHjtyNEcNTdO+e3Z9f6nDNzVBfDy+9BLfcknwYTjwRTjutIEKfLWlqgiVLkrlAbR9LliQh0baEAF27NxFqltDSbREbqhbRULGIuvAODWE1qVSasjIoK4NUWaR8+VJSC+ZRRgtlXcpJ7bs3Vf13o1tFN6orqulW0W2TR3XlxveqyquoLKvc4qMiVUF5qtxd/rRF2wp+yrf0ZjsdBrweY3yz9SZ/BE4Dthr8FIOZT7/IX+77Z67LkCRJJaX9f1C3pTPjFt7d0nsQ3ncs+TPCuMm12/7BYWz9CwLp1jPSMb53lXTmeIR06xmRSDpGWoikY/JuOibnpUmTjhvfb45pWmihOba0vk7TEluSZ5pppiX5Og3pGJJHekd+xTZVnq6ivKUb3Vu604Oe9Kvozm5dujOoazVDulUzvHsVg3oG+nTfQI9uza1bSS9PHm+RPKRc2fwP9TNfb/5+Op2kHfX1Gx/r12983bRZO9tJJ8GppxZs6ANQUQFDhyaPtmKE2tokBFq6NAmCVqxI3quthbVrk2Vk9bUVUDsUSC5Q2fqIpNkQ1rIhtZrG1ErqwyoaU6tpbF5C48rX2JBezob5tVA+n1QqmT0UUoGQCpt8nSoLhBSkQoAAgUgIG3/JA7z3dQUVlFFGWSgjFVPJMynKyDynSIXMV8lfgfDecyAQQmhzZNO/kvtt9jrQ+nVg0/3PQptXYZPn7Wl7Xqqd39N+7b/eZz/1UfYYNaKD759fdiX4GQy0HYe+EDh818rJf3PmvMaf3no012VIkiSpjVRMkUpXUp4up6ylgvJYTkUso7L10SWm6EIZ3SijRyynByl6hxS9CPRJJY++Kehe3kS38g30qKilqnxVcvHG1sfqHP6AUjalUlBdnexUNWlSMsy5gEOfbQkh2Ra+R49kCPSWtLTAunUbg6BMKFRbCw0NKRobe9HY2IuGhhE0Nibbzzc0QGNVpOG1BWyY/TYtoYHmsgaayxtpLlvf+px83dTm65ayDaRTzbSkmkinmt/3Oh1agA1bLlQ75UNvHWbws6tCCF8AvgAwbNiwzr5dpxszZm8+9vYHc12GJEnSVrX/T1vb9z2bnBfC+/5Ud+OfSCfvpMKmf3a8yXsh+dPishBIheRPm1MhUBaS4allAcpan1MhRWVIUZEqoyKVorIsRWUqRWUqUJlKvfdel1SKLuVQWZ6mojxSWZ5u7cSRStjmQc17H9TW565dk2Cna9ck5Gn7dWVl0QY9O6OsDHr2TB47JgDDSK8fSPOqWprXN9Fcv4GWhiaa1jfT0tBEc0Nz8n5DM80NzbQ0x/e6FtPpjV2M6XTSndTUEmmMzUkXJGlaYvJIt3ZGthCTLsiYTl7T0tp5mW7TWZl0V0JrJ2aMbfoxW/9qbRLLnLfxSMaWOj83FTd5veubSnXENbZk+B5DOuW6+WRXgp9FZPrcEkNa39tEjPF64HpIZvzswv3ywrgJBzBuwgG5LkOSJEmSVABSXbtQ2bULbuanXNmVPwt5Ftg7hLBHCKES+CRwT8eUJUmSJEmSpF210x0/McbmEMLFwIMk27nfFGOc02GVSZIkSZIkaZfs0oyfGON9wH0dVIskSZIkSZI6kGPvJEmSJEmSipTBjyRJkiRJUpEy+JEkSZIkSSpSBj+SJEmSJElFyuBHkiRJkiSpSBn8SJIkSZIkFSmDH0mSJEmSpCJl8CNJkiRJklSkDH4kSZIkSZKKlMGPJEmSJElSkTL4kSRJkiRJKlIGP5IkSZIkSUUqxBizd7MQlgFvZ+2GO64fsDzXRUgFxM+MtGP8zEg7xs+MtGP8zEg7ppg+M8NjjP23dCCrwU++CyFMjzGOz3UdUqHwMyPtGD8z0o7xMyPtGD8z0o4plc+MS70kSZIkSZKKlMGPJEmSJElSkTL42dT1uS5AKjB+ZqQd42dG2jF+ZqQd42dG2jEl8Zlxxo8kSZIkSVKRsuNHkiRJkiSpSBn8SJIkSZIkFamSDH5CCCeEEF4NIbweQvjWFo53CSHc0Xp8WghhRA7KlPJGOz4zXwshvBRCeDGE8HAIYXgu6pTyxfY+M23OOzOEEEMIRb+NqLQt7fnMhBA+0frfmjkhhNuzXaOUT9rx/2bDQgiPhBCeb/3/s5NyUaeUD0IIN4UQloYQZm/leAgh/LL18/RiCOHgbNfY2Uou+AkhlAG/Bk4E9gPODiHst9lp5wOrYox7AT8DfpzdKqX80c7PzPPA+BjjAcBdwE+yW6WUP9r5mSGEUANcCkzLboVSfmnPZyaEsDfwbeCoGOMY4KvZrlPKF+3878x/AHfGGA8CPglck90qpbxyM3DCNo6fCOzd+vgC8Jss1JRVJRf8AIcBr8cY34wxbgD+CJy22TmnAbe0vr4LOCaEELJYo5RPtvuZiTE+EmOsb/3yaWBIlmuU8kl7/jsD8H9J/mChIZvFSXmoPZ+ZzwO/jjGuAogxLs1yjVI+ac9nJgI9Wl/3BBZnsT4pr8QYHwdWbuOU04DfxcTTQK8QwqDsVJcdpRj8DAYWtPl6Yet7WzwnxtgMrAH6ZqU6Kf+05zPT1vnA/Z1akZTftvuZaW0hHhpj/Hs2C5PyVHv+O7MPsE8I4akQwtMhhG39ya1U7Nrzmfkv4NwQwkLgPuCS7JQmFaQd/f1OwSnPdQGSikcI4VxgPPDBXNci5asQQgr4KfBvOS5FKiTlJC34k0m6Sh8PIYyNMa7OZVFSHjsbuDnGeFUI4Qjg1hDC/jHGdK4Lk5R9pdjxswgY2ubrIa3vbfGcEEI5SXvkiqxUJ+Wf9nxmCCEcC3wXODXG2Jil2qR8tL3PTA2wP/BoCGEeMAG4xwHPKmHt+e/MQuCeGGNTjPEtYC5JECSVovZ8Zs4H7gSIMU4FqoB+WalOKjzt+v1OISvF4OdZYO8Qwh4hhEqSYWf3bHbOPcBnWl9/DPhXjDFmsUYpn2z3MxNCOAi4jiT0ce6CSt02PzMxxjUxxn4xxhExxhEkc7FOjTFOz025Us615//N/pek24cQQj+SpV9vZrFGKZ+05zMzHzgGIIQwmiT4WZbVKqXCcQ/w6dbdvSYAa2KM7+S6qI5Ucku9YozNIYSLgQeBMuCmGOOcEML3gekxxnuA35K0Q75OMgTqk7mrWMqtdn5mrgC6A39qnYM+P8Z4as6KlnKonZ8ZSa3a+Zl5EPhwCOEloAX4RozRbmyVpHZ+Zr4O3BBCuIxk0PO/+QfZKlUhhD+Q/OFBv9a5V/8HqACIMV5LMgfrJOB1oB74bG4q7TzBz78kSZIkSVJxKsWlXpIkSZIkSSXB4EeSJEmSJKlIGfxIkiRJkiQVKYMfSZIkSZKkImXwI0mSJEmSVKQMfiRJkiRJkoqUwY8kSZIkSVKR+v8BXrYzEg5pP3YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sc\n",
    "\n",
    "\n",
    "\n",
    "x = np.linspace(0.01,1, 100)\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "plt.plot(x, sc.norm.pdf(x,loc=0.859214,scale=0.011093),\n",
    "        'k-', lw=2, alpha=0.6, label='Training', color='r')\n",
    "\n",
    "plt.plot(x, sc.norm.pdf(x,loc=0.757231,scale=0.0),\n",
    "'k-', lw=2, alpha=0.6, label='Validation', color='b')\n",
    "\n",
    "plt.plot(x, sc.norm.pdf(x,loc=0.763987,scale=0.04823),\n",
    "'k-', lw=2, alpha=0.6, label='BCR', color='g')\n",
    "\n",
    "plt.legend([\"pdf of T_n-1\", \"t_obs\", \"t_a\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Competition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "a4c97918448b2f30896c96ac6fcc6e0acbe53b489c7c4eda4899ab81d4beed22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}